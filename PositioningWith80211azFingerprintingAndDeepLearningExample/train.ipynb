{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ae07a7-9241-40fe-bd45-013bd062aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996643e-08b9-4441-9791-9e29639b0dc7",
   "metadata": {},
   "source": [
    "#### Load matlab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f8402-11a0-47c8-9e97-33cb19abc744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e07e4c9-3d3b-4641-8a65-839623c8be82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3200, 1, 16, 48]), torch.Size([3200, 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = scipy.io.loadmat('output/feats4T.mat')\n",
    "labels = scipy.io.loadmat('output/labels4T.mat')\n",
    "\n",
    "feats = feats['features']\n",
    "labels = labels['lp']\n",
    "\n",
    "X = torch.Tensor(feats)\n",
    "Y = torch.Tensor(labels)\n",
    "\n",
    "Y = Y.T\n",
    "\n",
    "X = X.T\n",
    "X = torch.flatten(X, start_dim=1, end_dim=2)\n",
    "X = X[:, None, :, :]\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c9274-8716-4a5d-b65d-b4f665596153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e033ff87-8f4e-4f76-9e95-ab52566b8859",
   "metadata": {},
   "source": [
    "### sample shape: (minibatch_size, 1, 16, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a41f318-1a60-49df-aa03-0e3a70fcccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, Y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65c698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e06d19fa-4f93-4466-9439-b085664b0896",
   "metadata": {},
   "source": [
    "### Define models and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03cf0009-b224-4b40-a244-5803a0cef500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "        )\n",
    "        linear_in_dim = int(16/2/2/2*48/2/2/2*64)\n",
    "        self.linear1 = nn.Linear(linear_in_dim, 300)\n",
    "        # self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        # self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.linear3 = nn.Linear(100, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.seq(x)\n",
    "        out = out.view(out.size(0), -1) # flatten to (batch size, int)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        # out = self.dropout1(out)\n",
    "        out = F.relu(self.linear2(out))\n",
    "        # out = self.dropout2(out)\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "        \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(48, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(20, 10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(10, 3) # x,y,z outputs\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "def EucLoss(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    assert a.shape == b.shape\n",
    "    assert b.shape[-1] == 3\n",
    "    return torch.sum((a-b).square(), dim=-1).sqrt().mean()\n",
    "\n",
    "def EucLossSquared(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    assert a.shape == b.shape\n",
    "    assert b.shape[-1] == 3\n",
    "    return torch.sum((a-b).square(), dim=-1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae4d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348fcabd-f549-4bd5-8e8e-8a995acec81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCNN(\n",
       "  (seq): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=300, bias=True)\n",
       "  (linear2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MyCNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd1d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45247b73-15e4-44a8-b4e8-e737dff343bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = EucLoss\n",
    "# crit = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855d290-d4e8-4a60-ad2f-c23c10af4117",
   "metadata": {},
   "source": [
    "#### Training/Evaluating NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab6d8b7-884d-4d93-be50-8fe96ab82449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Training loss = 2.938876214623451 | Test loss = 2.5164523839950563\n",
      "Epoch 1 | Training loss = 2.585065132379532 | Test loss = 2.347435414791107\n",
      "Epoch 2 | Training loss = 2.388899825513363 | Test loss = 2.334470456838608\n",
      "Epoch 3 | Training loss = 2.3636202082037925 | Test loss = 2.2756114065647126\n",
      "Epoch 4 | Training loss = 2.1757480591535567 | Test loss = 2.093002754449844\n",
      "Epoch 5 | Training loss = 2.098741376399994 | Test loss = 2.637961578369141\n",
      "Epoch 6 | Training loss = 2.0059750601649284 | Test loss = 2.2452427983283996\n",
      "Epoch 7 | Training loss = 2.0261476680636408 | Test loss = 1.939679390192032\n",
      "Epoch 8 | Training loss = 1.9198524951934814 | Test loss = 1.8888706266880035\n",
      "Epoch 9 | Training loss = 1.836622415482998 | Test loss = 1.976983255147934\n",
      "Epoch 10 | Training loss = 1.7786316111683846 | Test loss = 1.8391382098197937\n",
      "Epoch 11 | Training loss = 1.7153554469347 | Test loss = 1.7981627643108369\n",
      "Epoch 12 | Training loss = 1.7134462252259255 | Test loss = 1.9205962061882018\n",
      "Epoch 13 | Training loss = 1.6437096729874612 | Test loss = 1.8999883592128755\n",
      "Epoch 14 | Training loss = 1.6217852726578712 | Test loss = 2.1046217501163484\n",
      "Epoch 15 | Training loss = 1.5839592933654785 | Test loss = 1.8419146180152892\n",
      "Epoch 16 | Training loss = 1.519334678351879 | Test loss = 1.7704569041728973\n",
      "Epoch 17 | Training loss = 1.53463813662529 | Test loss = 1.9852789044380188\n",
      "Epoch 18 | Training loss = 1.4284381672739983 | Test loss = 1.7459774494171143\n",
      "Epoch 19 | Training loss = 1.392920297384262 | Test loss = 1.8320100486278534\n",
      "Epoch 20 | Training loss = 1.3670133247971534 | Test loss = 2.038539582490921\n",
      "Epoch 21 | Training loss = 1.3004551358520984 | Test loss = 1.5666477739810944\n",
      "Epoch 22 | Training loss = 1.2540683306753635 | Test loss = 1.6634169280529023\n",
      "Epoch 23 | Training loss = 1.1928020372986794 | Test loss = 1.4983733117580413\n",
      "Epoch 24 | Training loss = 1.1454350426793098 | Test loss = 1.6777875304222107\n",
      "Epoch 25 | Training loss = 1.115435903519392 | Test loss = 1.5188377439975738\n",
      "Epoch 26 | Training loss = 1.1120074823498727 | Test loss = 1.515397423505783\n",
      "Epoch 27 | Training loss = 1.0431136816740036 | Test loss = 1.7461034178733825\n",
      "Epoch 28 | Training loss = 1.0561305068433284 | Test loss = 1.6637625098228455\n",
      "Epoch 29 | Training loss = 0.9954251751303673 | Test loss = 1.4626048445701598\n",
      "Epoch 30 | Training loss = 0.9387290678918362 | Test loss = 1.4951771795749664\n",
      "Epoch 31 | Training loss = 0.9390927225351333 | Test loss = 1.4628920435905457\n",
      "Epoch 32 | Training loss = 0.9121992997825146 | Test loss = 1.4516666412353516\n",
      "Epoch 33 | Training loss = 0.882129842787981 | Test loss = 1.5489822983741761\n",
      "Epoch 34 | Training loss = 0.8615272037684918 | Test loss = 1.5021036565303802\n",
      "Epoch 35 | Training loss = 0.8125903658568859 | Test loss = 1.525949513912201\n",
      "Epoch 36 | Training loss = 0.8313039295375347 | Test loss = 1.4874947607517242\n",
      "Epoch 37 | Training loss = 0.8190230146050453 | Test loss = 1.4597490727901459\n",
      "Epoch 38 | Training loss = 0.7895912051200866 | Test loss = 1.5257832765579225\n",
      "Epoch 39 | Training loss = 0.7290612205862999 | Test loss = 1.4393684089183807\n",
      "Epoch 40 | Training loss = 0.6836628146469593 | Test loss = 1.4280590415000916\n",
      "Epoch 41 | Training loss = 0.6751197703182698 | Test loss = 1.4223693907260895\n",
      "Epoch 42 | Training loss = 0.6472246613353491 | Test loss = 1.3644441723823548\n",
      "Epoch 43 | Training loss = 0.643853648006916 | Test loss = 1.3748526573181152\n",
      "Epoch 44 | Training loss = 0.6226582761853934 | Test loss = 1.428618198633194\n",
      "Epoch 45 | Training loss = 0.6082641866058112 | Test loss = 1.394081211090088\n",
      "Epoch 46 | Training loss = 0.6115010187029839 | Test loss = 1.4358782052993775\n",
      "Epoch 47 | Training loss = 0.5610616851598025 | Test loss = 1.3653262674808502\n",
      "Epoch 48 | Training loss = 0.556659483909607 | Test loss = 1.3675033390522002\n",
      "Epoch 49 | Training loss = 0.5339870829135179 | Test loss = 1.4472361505031586\n",
      "Epoch 50 | Training loss = 0.5003164604306221 | Test loss = 1.3610290467739106\n",
      "Epoch 51 | Training loss = 0.49553481973707675 | Test loss = 1.3475404322147368\n",
      "Epoch 52 | Training loss = 0.4727787468582392 | Test loss = 1.3690636038780213\n",
      "Epoch 53 | Training loss = 0.4761216133832932 | Test loss = 1.385496547818184\n",
      "Epoch 54 | Training loss = 0.46749872751533983 | Test loss = 1.3602811008691789\n",
      "Epoch 55 | Training loss = 0.45902764089405534 | Test loss = 1.378891408443451\n",
      "Epoch 56 | Training loss = 0.46773155592381954 | Test loss = 1.386182314157486\n",
      "Epoch 57 | Training loss = 0.4490343198180199 | Test loss = 1.382941472530365\n",
      "Epoch 58 | Training loss = 0.4185220949351788 | Test loss = 1.3692978084087373\n",
      "Epoch 59 | Training loss = 0.4053704310208559 | Test loss = 1.4016377210617066\n",
      "Epoch 60 | Training loss = 0.4014785122126341 | Test loss = 1.4035241544246673\n",
      "Epoch 61 | Training loss = 0.4084091816097498 | Test loss = 1.3647030532360076\n",
      "Epoch 62 | Training loss = 0.40896835923194885 | Test loss = 1.4102994203567505\n",
      "Epoch 63 | Training loss = 0.3904819957911968 | Test loss = 1.3810268491506577\n",
      "Epoch 64 | Training loss = 0.3769129324704409 | Test loss = 1.3698804438114167\n",
      "Epoch 65 | Training loss = 0.3646254774183035 | Test loss = 1.3974526464939117\n",
      "Epoch 66 | Training loss = 0.3690155431628227 | Test loss = 1.3731723129749298\n",
      "Epoch 67 | Training loss = 0.3703788761049509 | Test loss = 1.3634714424610137\n",
      "Epoch 68 | Training loss = 0.3565042868256569 | Test loss = 1.3551027059555054\n",
      "Epoch 69 | Training loss = 0.3339197613298893 | Test loss = 1.356646865606308\n",
      "Epoch 70 | Training loss = 0.32922726348042486 | Test loss = 1.3619274258613587\n",
      "Epoch 71 | Training loss = 0.3213504493236542 | Test loss = 1.367490828037262\n",
      "Epoch 72 | Training loss = 0.31715427823364734 | Test loss = 1.3823199331760407\n",
      "Epoch 73 | Training loss = 0.3100304789841175 | Test loss = 1.3747996747493745\n",
      "Epoch 74 | Training loss = 0.2904263762757182 | Test loss = 1.3582016408443451\n",
      "Epoch 75 | Training loss = 0.31015218179672954 | Test loss = 1.3878614783287049\n",
      "Epoch 76 | Training loss = 0.29958346020430326 | Test loss = 1.403092497587204\n",
      "Epoch 77 | Training loss = 0.3029607465490699 | Test loss = 1.3759984850883484\n",
      "Epoch 78 | Training loss = 0.28531050086021426 | Test loss = 1.347038334608078\n",
      "Epoch 79 | Training loss = 0.27638612743467095 | Test loss = 1.3790893316268922\n",
      "Epoch 80 | Training loss = 0.27987737506628035 | Test loss = 1.3594314694404601\n",
      "Epoch 81 | Training loss = 0.2727849058806896 | Test loss = 1.3461549401283264\n",
      "Epoch 82 | Training loss = 0.27202218435704706 | Test loss = 1.3748787105083466\n",
      "Epoch 83 | Training loss = 0.2696994308382273 | Test loss = 1.3312420070171356\n",
      "Epoch 84 | Training loss = 0.26404721196740866 | Test loss = 1.3277338147163391\n",
      "Epoch 85 | Training loss = 0.2547231946140528 | Test loss = 1.337868583202362\n",
      "Epoch 86 | Training loss = 0.24871525559574365 | Test loss = 1.350461971759796\n",
      "Epoch 87 | Training loss = 0.24358285702764987 | Test loss = 1.3649463087320328\n",
      "Epoch 88 | Training loss = 0.2492450799793005 | Test loss = 1.3682614803314208\n",
      "Epoch 89 | Training loss = 0.24799873363226652 | Test loss = 1.3366150975227356\n",
      "Epoch 90 | Training loss = 0.24267863109707832 | Test loss = 1.3720814764499665\n",
      "Epoch 91 | Training loss = 0.26281868014484644 | Test loss = 1.3317699313163758\n",
      "Epoch 92 | Training loss = 0.23642695136368275 | Test loss = 1.338070124387741\n",
      "Epoch 93 | Training loss = 0.23370759878307582 | Test loss = 1.3657779395580292\n",
      "Epoch 94 | Training loss = 0.22931871209293603 | Test loss = 1.3654694318771363\n",
      "Epoch 95 | Training loss = 0.23065241202712058 | Test loss = 1.361293339729309\n",
      "Epoch 96 | Training loss = 0.22051310241222383 | Test loss = 1.329093873500824\n",
      "Epoch 97 | Training loss = 0.21588262561708688 | Test loss = 1.332506451010704\n",
      "Epoch 98 | Training loss = 0.21366138830780984 | Test loss = 1.3647217988967895\n",
      "Epoch 99 | Training loss = 0.2238908600062132 | Test loss = 1.3542102307081223\n",
      "Epoch 100 | Training loss = 0.22856263183057307 | Test loss = 1.3313100576400756\n",
      "Epoch 101 | Training loss = 0.2045490389689803 | Test loss = 1.3343590795993805\n",
      "Epoch 102 | Training loss = 0.20974233653396368 | Test loss = 1.340251898765564\n",
      "Epoch 103 | Training loss = 0.21737894434481858 | Test loss = 1.3261140137910843\n",
      "Epoch 104 | Training loss = 0.2097479609772563 | Test loss = 1.3469390451908112\n",
      "Epoch 105 | Training loss = 0.21305313427001238 | Test loss = 1.359782302379608\n",
      "Epoch 106 | Training loss = 0.20852753072977065 | Test loss = 1.3464379608631134\n",
      "Epoch 107 | Training loss = 0.20041845962405205 | Test loss = 1.3415645003318786\n",
      "Epoch 108 | Training loss = 0.20778534561395645 | Test loss = 1.3353526771068573\n",
      "Epoch 109 | Training loss = 0.20639271661639214 | Test loss = 1.340109610557556\n",
      "Epoch 110 | Training loss = 0.21028915140777826 | Test loss = 1.3335465133190154\n",
      "Epoch 111 | Training loss = 0.19602837413549423 | Test loss = 1.3346582770347595\n",
      "Epoch 112 | Training loss = 0.19584060423076152 | Test loss = 1.3489325135946273\n",
      "Epoch 113 | Training loss = 0.19389714095741512 | Test loss = 1.346206670999527\n",
      "Epoch 114 | Training loss = 0.1914675559848547 | Test loss = 1.3488145828247071\n",
      "Epoch 115 | Training loss = 0.19100302681326867 | Test loss = 1.340360802412033\n",
      "Epoch 116 | Training loss = 0.1971752243116498 | Test loss = 1.3398774236440658\n",
      "Epoch 117 | Training loss = 0.191727976128459 | Test loss = 1.3461669981479645\n",
      "Epoch 118 | Training loss = 0.18680783957242966 | Test loss = 1.3425314962863921\n",
      "Epoch 119 | Training loss = 0.18537456057965757 | Test loss = 1.3519879937171937\n",
      "Epoch 120 | Training loss = 0.18858077060431241 | Test loss = 1.3250365823507309\n",
      "Epoch 121 | Training loss = 0.18786108251661063 | Test loss = 1.3140941977500915\n",
      "Epoch 122 | Training loss = 0.1880634507164359 | Test loss = 1.3198007345199585\n",
      "Epoch 123 | Training loss = 0.18853849451988935 | Test loss = 1.3422214329242705\n",
      "Epoch 124 | Training loss = 0.17729834280908108 | Test loss = 1.3301956593990325\n",
      "Epoch 125 | Training loss = 0.18240722771734 | Test loss = 1.3437647700309754\n",
      "Epoch 126 | Training loss = 0.17867262624204158 | Test loss = 1.349713110923767\n",
      "Epoch 127 | Training loss = 0.18223525322973727 | Test loss = 1.3467164278030395\n",
      "Epoch 128 | Training loss = 0.1771574007347226 | Test loss = 1.319838970899582\n",
      "Epoch 129 | Training loss = 0.1765075523406267 | Test loss = 1.3334736406803132\n",
      "Epoch 130 | Training loss = 0.17246659751981497 | Test loss = 1.3318154752254485\n",
      "Epoch 131 | Training loss = 0.17164519969373943 | Test loss = 1.3541915357112884\n",
      "Epoch 132 | Training loss = 0.167418765835464 | Test loss = 1.3267773866653443\n",
      "Epoch 133 | Training loss = 0.16367548871785403 | Test loss = 1.3189921885728837\n",
      "Epoch 134 | Training loss = 0.17056238390505313 | Test loss = 1.3222200989723205\n",
      "Epoch 135 | Training loss = 0.16267028786242008 | Test loss = 1.3272070825099944\n",
      "Epoch 136 | Training loss = 0.15905767269432544 | Test loss = 1.33355313539505\n",
      "Epoch 137 | Training loss = 0.17277796752750874 | Test loss = 1.3340568959712982\n",
      "Epoch 138 | Training loss = 0.1727614499628544 | Test loss = 1.3295601665973664\n",
      "Epoch 139 | Training loss = 0.1724938984028995 | Test loss = 1.3557138800621034\n",
      "Epoch 140 | Training loss = 0.158864385727793 | Test loss = 1.3285506546497345\n",
      "Epoch 141 | Training loss = 0.1589772374369204 | Test loss = 1.3245548367500306\n",
      "Epoch 142 | Training loss = 0.1638537553139031 | Test loss = 1.3316103726625443\n",
      "Epoch 143 | Training loss = 0.16068508438766002 | Test loss = 1.341964989900589\n",
      "Epoch 144 | Training loss = 0.1671191392466426 | Test loss = 1.3322879135608674\n",
      "Epoch 145 | Training loss = 0.1614876240491867 | Test loss = 1.3423823058605193\n",
      "Epoch 146 | Training loss = 0.15853801751509308 | Test loss = 1.3693513691425323\n",
      "Epoch 147 | Training loss = 0.16059410413727165 | Test loss = 1.3222250461578369\n",
      "Epoch 148 | Training loss = 0.15398751739412547 | Test loss = 1.3337899595499039\n",
      "Epoch 149 | Training loss = 0.1565719410777092 | Test loss = 1.346818494796753\n",
      "Epoch 150 | Training loss = 0.1505644153803587 | Test loss = 1.3503852844238282\n",
      "Epoch 151 | Training loss = 0.16198030104860664 | Test loss = 1.3419019758701325\n",
      "Epoch 152 | Training loss = 0.15234482223168017 | Test loss = 1.3524134904146194\n",
      "Epoch 153 | Training loss = 0.1614637551829219 | Test loss = 1.3270836889743804\n",
      "Epoch 154 | Training loss = 0.15183486687019468 | Test loss = 1.329022040963173\n",
      "Epoch 155 | Training loss = 0.151302810292691 | Test loss = 1.3414786636829377\n",
      "Epoch 156 | Training loss = 0.15046287830919028 | Test loss = 1.3281309962272645\n",
      "Epoch 157 | Training loss = 0.14894310431554914 | Test loss = 1.3398194849491118\n",
      "Epoch 158 | Training loss = 0.1451155649498105 | Test loss = 1.3253790199756623\n",
      "Epoch 159 | Training loss = 0.14521307414397597 | Test loss = 1.3337994635105133\n",
      "Epoch 160 | Training loss = 0.14067490184679626 | Test loss = 1.3289758086204528\n",
      "Epoch 161 | Training loss = 0.15211474308744072 | Test loss = 1.3279656797647477\n",
      "Epoch 162 | Training loss = 0.15002774382010103 | Test loss = 1.3382183730602264\n",
      "Epoch 163 | Training loss = 0.14783506374806166 | Test loss = 1.3309767693281174\n",
      "Epoch 164 | Training loss = 0.1484095362946391 | Test loss = 1.3437238603830337\n",
      "Epoch 165 | Training loss = 0.14261059351265432 | Test loss = 1.3308826208114624\n",
      "Epoch 166 | Training loss = 0.14417768381536006 | Test loss = 1.3364319920539856\n",
      "Epoch 167 | Training loss = 0.14019232913851737 | Test loss = 1.3326158463954925\n",
      "Epoch 168 | Training loss = 0.14056007657200098 | Test loss = 1.3416885495185853\n",
      "Epoch 169 | Training loss = 0.1382528254762292 | Test loss = 1.326008176803589\n",
      "Epoch 170 | Training loss = 0.1423077386803925 | Test loss = 1.324620109796524\n",
      "Epoch 171 | Training loss = 0.14070544382557273 | Test loss = 1.3517525851726533\n",
      "Epoch 172 | Training loss = 0.13923833817243575 | Test loss = 1.3375643908977508\n",
      "Epoch 173 | Training loss = 0.13976986510679126 | Test loss = 1.3335804373025895\n",
      "Epoch 174 | Training loss = 0.15020217830315233 | Test loss = 1.35454863011837\n",
      "Epoch 175 | Training loss = 0.14321016045287252 | Test loss = 1.3434841722249984\n",
      "Epoch 176 | Training loss = 0.14089339599013329 | Test loss = 1.3462222576141358\n",
      "Epoch 177 | Training loss = 0.14165789233520626 | Test loss = 1.345540201663971\n",
      "Epoch 178 | Training loss = 0.13969681290909647 | Test loss = 1.3359624087810515\n",
      "Epoch 179 | Training loss = 0.1430343646556139 | Test loss = 1.3270896792411804\n",
      "Epoch 180 | Training loss = 0.1341946148313582 | Test loss = 1.3517486661672593\n",
      "Epoch 181 | Training loss = 0.13357734885066747 | Test loss = 1.3459809124469757\n",
      "Epoch 182 | Training loss = 0.13465981986373662 | Test loss = 1.3413811028003693\n",
      "Epoch 183 | Training loss = 0.1303845121525228 | Test loss = 1.34552241563797\n",
      "Epoch 184 | Training loss = 0.1396563441492617 | Test loss = 1.3411763310432434\n",
      "Epoch 185 | Training loss = 0.13446080824360251 | Test loss = 1.3251630008220672\n",
      "Epoch 186 | Training loss = 0.1353399540297687 | Test loss = 1.3562387824058533\n",
      "Epoch 187 | Training loss = 0.14153056275099515 | Test loss = 1.3258626818656922\n",
      "Epoch 188 | Training loss = 0.13348171692341565 | Test loss = 1.3328262388706207\n",
      "Epoch 189 | Training loss = 0.14213915690779685 | Test loss = 1.3430113792419434\n",
      "Epoch 190 | Training loss = 0.13860506778582932 | Test loss = 1.3359946429729461\n",
      "Epoch 191 | Training loss = 0.13660358376801013 | Test loss = 1.337873739004135\n",
      "Epoch 192 | Training loss = 0.13156735040247441 | Test loss = 1.3392788469791412\n",
      "Epoch 193 | Training loss = 0.12603888381272554 | Test loss = 1.331049743294716\n",
      "Epoch 194 | Training loss = 0.12571060182526708 | Test loss = 1.3284839928150176\n",
      "Epoch 195 | Training loss = 0.12858397774398328 | Test loss = 1.3275812089443206\n",
      "Epoch 196 | Training loss = 0.12979707233607768 | Test loss = 1.3368072867393495\n",
      "Epoch 197 | Training loss = 0.12622450161725282 | Test loss = 1.3398160994052888\n",
      "Epoch 198 | Training loss = 0.12922394340857862 | Test loss = 1.333030891418457\n",
      "Epoch 199 | Training loss = 0.12483972208574415 | Test loss = 1.3268355876207352\n",
      "Epoch 200 | Training loss = 0.12478791093453764 | Test loss = 1.3273097038269044\n",
      "Epoch 201 | Training loss = 0.1252918946556747 | Test loss = 1.3424006700515747\n",
      "Epoch 202 | Training loss = 0.12338646231219172 | Test loss = 1.3271719217300415\n",
      "Epoch 203 | Training loss = 0.12636870788410307 | Test loss = 1.3429503202438355\n",
      "Epoch 204 | Training loss = 0.1247362218797207 | Test loss = 1.3454019486904145\n",
      "Epoch 205 | Training loss = 0.1233798242174089 | Test loss = 1.3338231682777404\n",
      "Epoch 206 | Training loss = 0.1293159045279026 | Test loss = 1.3536994099617004\n",
      "Epoch 207 | Training loss = 0.12769962726160883 | Test loss = 1.3240485310554504\n",
      "Epoch 208 | Training loss = 0.12404269259423018 | Test loss = 1.3244662702083587\n",
      "Epoch 209 | Training loss = 0.12285829754546285 | Test loss = 1.3368768125772477\n",
      "Epoch 210 | Training loss = 0.12250376213341951 | Test loss = 1.3484013259410859\n",
      "Epoch 211 | Training loss = 0.13050376260653138 | Test loss = 1.327575784921646\n",
      "Epoch 212 | Training loss = 0.13314242428168654 | Test loss = 1.3483368337154389\n",
      "Epoch 213 | Training loss = 0.12536527970805764 | Test loss = 1.3340703666210174\n",
      "Epoch 214 | Training loss = 0.11963376915082335 | Test loss = 1.3377256870269776\n",
      "Epoch 215 | Training loss = 0.1231153872795403 | Test loss = 1.3415778398513794\n",
      "Epoch 216 | Training loss = 0.12248633233830333 | Test loss = 1.3400273203849793\n",
      "Epoch 217 | Training loss = 0.1215325023047626 | Test loss = 1.3337588369846345\n",
      "Epoch 218 | Training loss = 0.11835038755089045 | Test loss = 1.3317412853240966\n",
      "Epoch 219 | Training loss = 0.12346279518678785 | Test loss = 1.3441903471946717\n",
      "Epoch 220 | Training loss = 0.12031450709328055 | Test loss = 1.3405309319496155\n",
      "Epoch 221 | Training loss = 0.12184536391869187 | Test loss = 1.3416457504034043\n",
      "Epoch 222 | Training loss = 0.11520413802936673 | Test loss = 1.3324179202318192\n",
      "Epoch 223 | Training loss = 0.11981815500184893 | Test loss = 1.3366505801677704\n",
      "Epoch 224 | Training loss = 0.1184001893736422 | Test loss = 1.3223360061645508\n",
      "Epoch 225 | Training loss = 0.11875841291621328 | Test loss = 1.3510081231594087\n",
      "Epoch 226 | Training loss = 0.11987882647663355 | Test loss = 1.3181113839149474\n",
      "Epoch 227 | Training loss = 0.11627257578074932 | Test loss = 1.326354530453682\n",
      "Epoch 228 | Training loss = 0.11590039646252989 | Test loss = 1.3267046868801118\n",
      "Epoch 229 | Training loss = 0.11930507868528366 | Test loss = 1.3245535790920258\n",
      "Epoch 230 | Training loss = 0.12013408346101642 | Test loss = 1.3244603097438812\n",
      "Epoch 231 | Training loss = 0.11517469491809607 | Test loss = 1.3345612853765487\n",
      "Epoch 232 | Training loss = 0.11649369103834033 | Test loss = 1.3383253782987594\n",
      "Epoch 233 | Training loss = 0.11805402282625437 | Test loss = 1.3389583826065063\n",
      "Epoch 234 | Training loss = 0.11533323042094708 | Test loss = 1.341863566637039\n",
      "Epoch 235 | Training loss = 0.11184687446802855 | Test loss = 1.3250104308128356\n",
      "Epoch 236 | Training loss = 0.11576666664332151 | Test loss = 1.3436346352100372\n",
      "Epoch 237 | Training loss = 0.11646677805110812 | Test loss = 1.339402687549591\n",
      "Epoch 238 | Training loss = 0.11951869968324899 | Test loss = 1.3228103995323182\n",
      "Epoch 239 | Training loss = 0.11023063259199262 | Test loss = 1.3294137477874757\n",
      "Epoch 240 | Training loss = 0.11103313537314534 | Test loss = 1.3433081716299058\n",
      "Epoch 241 | Training loss = 0.11145563069730997 | Test loss = 1.3449313282966613\n",
      "Epoch 242 | Training loss = 0.10741042280569672 | Test loss = 1.3329013109207153\n",
      "Epoch 243 | Training loss = 0.11002193326130509 | Test loss = 1.3267809569835662\n",
      "Epoch 244 | Training loss = 0.11415677629411221 | Test loss = 1.3365789949893951\n",
      "Epoch 245 | Training loss = 0.11520592970773577 | Test loss = 1.3253013908863067\n",
      "Epoch 246 | Training loss = 0.114109440241009 | Test loss = 1.3561169981956482\n",
      "Epoch 247 | Training loss = 0.1115252586081624 | Test loss = 1.3310190051794053\n",
      "Epoch 248 | Training loss = 0.11383559498935938 | Test loss = 1.3314027577638625\n",
      "Epoch 249 | Training loss = 0.11217951560392976 | Test loss = 1.3443420946598053\n",
      "Epoch 250 | Training loss = 0.11045095091685653 | Test loss = 1.3309086620807649\n",
      "Epoch 251 | Training loss = 0.10960606848821045 | Test loss = 1.339348202943802\n",
      "Epoch 252 | Training loss = 0.11173934573307634 | Test loss = 1.3518910467624665\n",
      "Epoch 253 | Training loss = 0.10769325820729136 | Test loss = 1.344833791255951\n",
      "Epoch 254 | Training loss = 0.11056790836155414 | Test loss = 1.3390131533145904\n",
      "Epoch 255 | Training loss = 0.11231062887236476 | Test loss = 1.3407965511083604\n",
      "Epoch 256 | Training loss = 0.11153140254318714 | Test loss = 1.3408515870571136\n",
      "Epoch 257 | Training loss = 0.11024920549243689 | Test loss = 1.330205684900284\n",
      "Epoch 258 | Training loss = 0.11087798727676272 | Test loss = 1.3282406866550445\n",
      "Epoch 259 | Training loss = 0.1130408862605691 | Test loss = 1.3387367010116578\n",
      "Epoch 260 | Training loss = 0.10837676953524351 | Test loss = 1.334770542383194\n",
      "Epoch 261 | Training loss = 0.10799526954069734 | Test loss = 1.3333100497722625\n",
      "Epoch 262 | Training loss = 0.10358140552416444 | Test loss = 1.3169582694768907\n",
      "Epoch 263 | Training loss = 0.10860939137637615 | Test loss = 1.3449217855930329\n",
      "Epoch 264 | Training loss = 0.10863400185480714 | Test loss = 1.330138224363327\n",
      "Epoch 265 | Training loss = 0.10519706597551703 | Test loss = 1.3369663834571839\n",
      "Epoch 266 | Training loss = 0.10892089614644647 | Test loss = 1.3506918907165528\n",
      "Epoch 267 | Training loss = 0.10875455215573311 | Test loss = 1.3250569701194763\n",
      "Epoch 268 | Training loss = 0.10437154425308108 | Test loss = 1.3306412398815155\n",
      "Epoch 269 | Training loss = 0.10789549117907882 | Test loss = 1.3491006553173066\n",
      "Epoch 270 | Training loss = 0.10891994759440422 | Test loss = 1.3345024853944778\n",
      "Epoch 271 | Training loss = 0.10295501854270697 | Test loss = 1.323368862271309\n",
      "Epoch 272 | Training loss = 0.11279437048360705 | Test loss = 1.3294469177722932\n",
      "Epoch 273 | Training loss = 0.11348652420565486 | Test loss = 1.3280080378055572\n",
      "Epoch 274 | Training loss = 0.1103128501214087 | Test loss = 1.343401849269867\n",
      "Epoch 275 | Training loss = 0.1014154365286231 | Test loss = 1.3441704213619232\n",
      "Epoch 276 | Training loss = 0.10932445786893367 | Test loss = 1.328519243001938\n",
      "Epoch 277 | Training loss = 0.09847218096256256 | Test loss = 1.3336796700954436\n",
      "Epoch 278 | Training loss = 0.10347555540502071 | Test loss = 1.338820332288742\n",
      "Epoch 279 | Training loss = 0.10718319090083242 | Test loss = 1.3439207077026367\n",
      "Epoch 280 | Training loss = 0.10380721809342504 | Test loss = 1.351865977048874\n",
      "Epoch 281 | Training loss = 0.10239770002663136 | Test loss = 1.3497791230678557\n",
      "Epoch 282 | Training loss = 0.10147732626646758 | Test loss = 1.341644886136055\n",
      "Epoch 283 | Training loss = 0.09906984502449631 | Test loss = 1.3420478641986846\n",
      "Epoch 284 | Training loss = 0.10611385619267821 | Test loss = 1.3379622220993042\n",
      "Epoch 285 | Training loss = 0.100133076030761 | Test loss = 1.336260861158371\n",
      "Epoch 286 | Training loss = 0.09926264928653836 | Test loss = 1.3382000833749772\n",
      "Epoch 287 | Training loss = 0.10684965243563056 | Test loss = 1.3419893503189086\n",
      "Epoch 288 | Training loss = 0.11015666974708438 | Test loss = 1.3353419125080108\n",
      "Epoch 289 | Training loss = 0.10577737232670188 | Test loss = 1.335112926363945\n",
      "Epoch 290 | Training loss = 0.10079540982842446 | Test loss = 1.340269324183464\n",
      "Epoch 291 | Training loss = 0.1010079407133162 | Test loss = 1.3474804639816285\n",
      "Epoch 292 | Training loss = 0.10159689979627728 | Test loss = 1.344385015964508\n",
      "Epoch 293 | Training loss = 0.10478364750742912 | Test loss = 1.3456630676984787\n",
      "Epoch 294 | Training loss = 0.10548644019290805 | Test loss = 1.350414976477623\n",
      "Epoch 295 | Training loss = 0.1023622059263289 | Test loss = 1.3332311868667603\n",
      "Epoch 296 | Training loss = 0.10111831557005643 | Test loss = 1.3352433562278747\n",
      "Epoch 297 | Training loss = 0.09981748620048166 | Test loss = 1.3291266918182374\n",
      "Epoch 298 | Training loss = 0.10441911350935698 | Test loss = 1.3446349054574966\n",
      "Epoch 299 | Training loss = 0.09982888754457235 | Test loss = 1.325832450389862\n",
      "Epoch 300 | Training loss = 0.09961553895846009 | Test loss = 1.3519437223672868\n",
      "Epoch 301 | Training loss = 0.09968424430117011 | Test loss = 1.3363986134529113\n",
      "Epoch 302 | Training loss = 0.1018359468318522 | Test loss = 1.343405270576477\n",
      "Epoch 303 | Training loss = 0.10219084583222866 | Test loss = 1.34553062915802\n",
      "Epoch 304 | Training loss = 0.10374786676838994 | Test loss = 1.3588659524917603\n",
      "Epoch 305 | Training loss = 0.1044809442013502 | Test loss = 1.3463602423667909\n",
      "Epoch 306 | Training loss = 0.10792016573250293 | Test loss = 1.3527131140232087\n",
      "Epoch 307 | Training loss = 0.10143173346295953 | Test loss = 1.3428811490535737\n",
      "Epoch 308 | Training loss = 0.1047001201659441 | Test loss = 1.3313479781150819\n",
      "Epoch 309 | Training loss = 0.09985410030931234 | Test loss = 1.3354109466075896\n",
      "Epoch 310 | Training loss = 0.09580784766003489 | Test loss = 1.3283554911613464\n",
      "Epoch 311 | Training loss = 0.10365475937724114 | Test loss = 1.349329400062561\n",
      "Epoch 312 | Training loss = 0.09932379936799407 | Test loss = 1.31943798661232\n",
      "Epoch 313 | Training loss = 0.09270967505872249 | Test loss = 1.3505852103233338\n",
      "Epoch 314 | Training loss = 0.09966542366892099 | Test loss = 1.3390314519405364\n",
      "Epoch 315 | Training loss = 0.09830796029418706 | Test loss = 1.3365778148174285\n",
      "Epoch 316 | Training loss = 0.09591874293982983 | Test loss = 1.3365762829780579\n",
      "Epoch 317 | Training loss = 0.10071072382852435 | Test loss = 1.356699925661087\n",
      "Epoch 318 | Training loss = 0.10495435409247875 | Test loss = 1.3334444493055344\n",
      "Epoch 319 | Training loss = 0.10470293713733554 | Test loss = 1.3437382876873016\n",
      "Epoch 320 | Training loss = 0.09901696518063545 | Test loss = 1.3445567846298219\n",
      "Epoch 321 | Training loss = 0.10139249004423619 | Test loss = 1.3526777386665345\n",
      "Epoch 322 | Training loss = 0.0951539134606719 | Test loss = 1.3370411247015\n",
      "Epoch 323 | Training loss = 0.09398622810840607 | Test loss = 1.3362730622291565\n",
      "Epoch 324 | Training loss = 0.09452243959531188 | Test loss = 1.3301643699407577\n",
      "Epoch 325 | Training loss = 0.09967517489567398 | Test loss = 1.3345354974269867\n",
      "Epoch 326 | Training loss = 0.0929965291172266 | Test loss = 1.3435730695724488\n",
      "Epoch 327 | Training loss = 0.09911173023283482 | Test loss = 1.3453027546405791\n",
      "Epoch 328 | Training loss = 0.09599655102938413 | Test loss = 1.3409360826015473\n",
      "Epoch 329 | Training loss = 0.09889558125287294 | Test loss = 1.3385458409786224\n",
      "Epoch 330 | Training loss = 0.09372113114222884 | Test loss = 1.3220918118953704\n",
      "Epoch 331 | Training loss = 0.10301339654251933 | Test loss = 1.3365077435970307\n",
      "Epoch 332 | Training loss = 0.10369811300188303 | Test loss = 1.331091156601906\n",
      "Epoch 333 | Training loss = 0.09409254873171449 | Test loss = 1.3371834635734559\n",
      "Epoch 334 | Training loss = 0.09410064509138465 | Test loss = 1.3345821380615235\n",
      "Epoch 335 | Training loss = 0.09751631291583181 | Test loss = 1.3483195900917053\n",
      "Epoch 336 | Training loss = 0.09311005510389805 | Test loss = 1.3426226019859313\n",
      "Epoch 337 | Training loss = 0.09194927541539073 | Test loss = 1.3204341173171996\n",
      "Epoch 338 | Training loss = 0.09219166161492467 | Test loss = 1.3181761920452117\n",
      "Epoch 339 | Training loss = 0.09750253977254034 | Test loss = 1.351427584886551\n",
      "Epoch 340 | Training loss = 0.0962759705260396 | Test loss = 1.3354750573635101\n",
      "Epoch 341 | Training loss = 0.09693420976400376 | Test loss = 1.3375185430049896\n",
      "Epoch 342 | Training loss = 0.09608101481571793 | Test loss = 1.3262245416641236\n",
      "Epoch 343 | Training loss = 0.09552000612020492 | Test loss = 1.3283738255500794\n",
      "Epoch 344 | Training loss = 0.09159558555111288 | Test loss = 1.338960701227188\n",
      "Epoch 345 | Training loss = 0.09378092484548688 | Test loss = 1.3295127987861632\n",
      "Epoch 346 | Training loss = 0.09179361434653402 | Test loss = 1.3310912728309632\n",
      "Epoch 347 | Training loss = 0.09432795653119683 | Test loss = 1.3265082955360412\n",
      "Epoch 348 | Training loss = 0.0983479393646121 | Test loss = 1.3247170746326447\n",
      "Epoch 349 | Training loss = 0.0920395023189485 | Test loss = 1.3269664704799653\n",
      "Epoch 350 | Training loss = 0.09563275035470724 | Test loss = 1.3410067021846772\n",
      "Epoch 351 | Training loss = 0.09403938427567482 | Test loss = 1.335357230901718\n",
      "Epoch 352 | Training loss = 0.09508550027385354 | Test loss = 1.3415222585201263\n",
      "Epoch 353 | Training loss = 0.09064200548455119 | Test loss = 1.3448760211467743\n",
      "Epoch 354 | Training loss = 0.0926315151154995 | Test loss = 1.3339206904172898\n",
      "Epoch 355 | Training loss = 0.09114143885672092 | Test loss = 1.3319771349430085\n",
      "Epoch 356 | Training loss = 0.09016314055770636 | Test loss = 1.350085163116455\n",
      "Epoch 357 | Training loss = 0.09880648590624333 | Test loss = 1.3469161629676818\n",
      "Epoch 358 | Training loss = 0.09587345803156495 | Test loss = 1.3355185210704803\n",
      "Epoch 359 | Training loss = 0.0917310245335102 | Test loss = 1.3426722049713136\n",
      "Epoch 360 | Training loss = 0.09262990420684218 | Test loss = 1.3455921530723571\n",
      "Epoch 361 | Training loss = 0.09494749447330833 | Test loss = 1.3458157062530518\n",
      "Epoch 362 | Training loss = 0.09675360089167953 | Test loss = 1.356063386797905\n",
      "Epoch 363 | Training loss = 0.09052423071116208 | Test loss = 1.3310925960540771\n",
      "Epoch 364 | Training loss = 0.09292097575962543 | Test loss = 1.3356330573558808\n",
      "Epoch 365 | Training loss = 0.0930128307081759 | Test loss = 1.3488060623407363\n",
      "Epoch 366 | Training loss = 0.08694633962586522 | Test loss = 1.3283714771270752\n",
      "Epoch 367 | Training loss = 0.09126637522131205 | Test loss = 1.3541743814945222\n",
      "Epoch 368 | Training loss = 0.08903438244014979 | Test loss = 1.3367691516876221\n",
      "Epoch 369 | Training loss = 0.08685103207826614 | Test loss = 1.3288383781909943\n",
      "Epoch 370 | Training loss = 0.09489046083763242 | Test loss = 1.337990415096283\n",
      "Epoch 371 | Training loss = 0.09640285666100681 | Test loss = 1.3446341782808304\n",
      "Epoch 372 | Training loss = 0.09134808029048144 | Test loss = 1.3379712998867035\n",
      "Epoch 373 | Training loss = 0.08973247427493333 | Test loss = 1.339115357398987\n",
      "Epoch 374 | Training loss = 0.09047376653179526 | Test loss = 1.3449986815452575\n",
      "Epoch 375 | Training loss = 0.08952160384505987 | Test loss = 1.340229433774948\n",
      "Epoch 376 | Training loss = 0.0880474302917719 | Test loss = 1.3428261995315551\n",
      "Epoch 377 | Training loss = 0.08632781100459397 | Test loss = 1.3501893877983093\n",
      "Epoch 378 | Training loss = 0.08747776132076979 | Test loss = 1.3431278347969056\n",
      "Epoch 379 | Training loss = 0.0932170738466084 | Test loss = 1.3335591673851013\n",
      "Epoch 380 | Training loss = 0.08695155754685402 | Test loss = 1.3391038954257966\n",
      "Epoch 381 | Training loss = 0.08530137548223138 | Test loss = 1.3359056293964386\n",
      "Epoch 382 | Training loss = 0.08844891600310803 | Test loss = 1.3334901213645936\n",
      "Epoch 383 | Training loss = 0.08990608244203031 | Test loss = 1.3308500468730926\n",
      "Epoch 384 | Training loss = 0.08938125474378467 | Test loss = 1.3366235554218293\n",
      "Epoch 385 | Training loss = 0.08512332402169705 | Test loss = 1.3335318624973298\n",
      "Epoch 386 | Training loss = 0.08781549977138639 | Test loss = 1.348880535364151\n",
      "Epoch 387 | Training loss = 0.08687568809837103 | Test loss = 1.3474808037281036\n",
      "Epoch 388 | Training loss = 0.0898839340545237 | Test loss = 1.3415495872497558\n",
      "Epoch 389 | Training loss = 0.0898181644268334 | Test loss = 1.351561391353607\n",
      "Epoch 390 | Training loss = 0.08622780004516244 | Test loss = 1.3375806510448456\n",
      "Epoch 391 | Training loss = 0.08622583132237197 | Test loss = 1.351450079679489\n",
      "Epoch 392 | Training loss = 0.08696609642356634 | Test loss = 1.3460621237754822\n",
      "Epoch 393 | Training loss = 0.09154426082968711 | Test loss = 1.3735011756420135\n",
      "Epoch 394 | Training loss = 0.08876000577583909 | Test loss = 1.3294192880392075\n",
      "Epoch 395 | Training loss = 0.08732507713139057 | Test loss = 1.3459742039442062\n",
      "Epoch 396 | Training loss = 0.08378139166161418 | Test loss = 1.3506854712963103\n",
      "Epoch 397 | Training loss = 0.08782206904143094 | Test loss = 1.3419932782649995\n",
      "Epoch 398 | Training loss = 0.08752705352380871 | Test loss = 1.3427716314792633\n",
      "Epoch 399 | Training loss = 0.08960057692602277 | Test loss = 1.3512114703655242\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 400\n",
    "loss_tracker = np.zeros((num_epochs, 2))\n",
    "\n",
    "num_train_batches = len(train_loader)\n",
    "num_test_batches = len(test_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    for batch_idx, (ft, lbl) in enumerate(train_loader):\n",
    "        ft, lbl = ft.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(ft)\n",
    "        loss = crit(output, lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * ft.shape[0]\n",
    "    train_loss /= len(train_dataset)\n",
    "    loss_tracker[epoch, 0] = train_loss\n",
    "        \n",
    "        \n",
    "    model = model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ft, lbl) in enumerate(test_loader):\n",
    "            ft, lbl = ft.to(device), lbl.to(device)\n",
    "            output = model(ft)\n",
    "            loss = crit(output, lbl)\n",
    "            test_loss += loss.item() * ft.shape[0]\n",
    "    test_loss /= len(test_dataset)\n",
    "    loss_tracker[epoch, 1] = test_loss\n",
    "            \n",
    "    print('Epoch {} | Training loss = {} | Test loss = {}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1262ce8",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "CNN seemed to help accuracy, as well as more linear layers. However, it is overfitting heavily. Batchnorm didn't really make a difference, dropout seems to make things worse. try running on stampede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f333c6-a3e6-491a-b71c-d9ddae7c8b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_dir = Path('./loss_plots')\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_tracker)\n",
    "plt.title('Training vs Testing Loss (Mean Loss Per Batch)')\n",
    "plt.legend(['Training loss', 'Test loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(img_dir / 'large_dataset_9cm_overfit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c95018b-5f35-43fb-96b4-4289d5078dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_dir = Path('./models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), model_dir / 'large_dataset_9cm_overfit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b856b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
