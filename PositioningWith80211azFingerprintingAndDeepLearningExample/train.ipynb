{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ae07a7-9241-40fe-bd45-013bd062aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matlab.engine\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996643e-08b9-4441-9791-9e29639b0dc7",
   "metadata": {},
   "source": [
    "#### Load matlab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f8402-11a0-47c8-9e97-33cb19abc744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e07e4c9-3d3b-4641-8a65-839623c8be82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([480, 1, 16, 48]), torch.Size([480, 3]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = scipy.io.loadmat('output/feats4T.mat')\n",
    "labels = scipy.io.loadmat('output/labels4T.mat')\n",
    "\n",
    "feats = feats['features']\n",
    "labels = labels['lp']\n",
    "\n",
    "X = torch.Tensor(feats)\n",
    "Y = torch.Tensor(labels)\n",
    "\n",
    "Y = Y.T\n",
    "\n",
    "X = X.T\n",
    "X = torch.flatten(X, start_dim=1, end_dim=2)\n",
    "X = X[:, None, :, :]\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c9274-8716-4a5d-b65d-b4f665596153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e033ff87-8f4e-4f76-9e95-ab52566b8859",
   "metadata": {},
   "source": [
    "### sample shape: (minibatch_size, 1, 16, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a41f318-1a60-49df-aa03-0e3a70fcccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, Y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65c698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e06d19fa-4f93-4466-9439-b085664b0896",
   "metadata": {},
   "source": [
    "### Define models and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03cf0009-b224-4b40-a244-5803a0cef500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "        )\n",
    "        linear_in_dim = int(16/2/2/2*48/2/2/2*64)\n",
    "        self.linear1 = nn.Linear(linear_in_dim, 300)\n",
    "        # self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.linear2 = nn.Linear(300, 100)\n",
    "        # self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.linear3 = nn.Linear(100, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.seq(x)\n",
    "        out = out.view(out.size(0), -1) # flatten to (batch size, int)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        # out = self.dropout1(out)\n",
    "        out = F.relu(self.linear2(out))\n",
    "        # out = self.dropout2(out)\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "        \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(48, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(20, 10),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(10, 3) # x,y,z outputs\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "def EucLoss(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    assert a.shape == b.shape\n",
    "    assert b.shape[-1] == 3\n",
    "    return torch.sum((a-b).square(), dim=-1).sqrt().mean()\n",
    "\n",
    "def EucLossSquared(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    assert a.shape == b.shape\n",
    "    assert b.shape[-1] == 3\n",
    "    return torch.sum((a-b).square(), dim=-1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae4d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "348fcabd-f549-4bd5-8e8e-8a995acec81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCNN(\n",
       "  (seq): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=300, bias=True)\n",
       "  (linear2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MyCNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd1d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45247b73-15e4-44a8-b4e8-e737dff343bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = EucLossSquared\n",
    "# crit = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855d290-d4e8-4a60-ad2f-c23c10af4117",
   "metadata": {},
   "source": [
    "#### Training/Evaluating NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab6d8b7-884d-4d93-be50-8fe96ab82449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Training loss = 29.011784474054974 | Test loss = 15.97208563486735\n",
      "Epoch 1 | Training loss = 9.03323515256246 | Test loss = 8.097903569539389\n",
      "Epoch 2 | Training loss = 7.591636459032695 | Test loss = 8.240503152211508\n",
      "Epoch 3 | Training loss = 7.569251338640849 | Test loss = 7.9901580810546875\n",
      "Epoch 4 | Training loss = 7.441519180933635 | Test loss = 8.000683466593424\n",
      "Epoch 5 | Training loss = 7.412812232971191 | Test loss = 7.938380718231201\n",
      "Epoch 6 | Training loss = 7.328917940457662 | Test loss = 7.870643138885498\n",
      "Epoch 7 | Training loss = 7.306413650512695 | Test loss = 7.852035681406657\n",
      "Epoch 8 | Training loss = 6.991207281748454 | Test loss = 7.693150361378987\n",
      "Epoch 9 | Training loss = 6.86484173933665 | Test loss = 8.095197836558023\n",
      "Epoch 10 | Training loss = 6.78427787621816 | Test loss = 7.73173729578654\n",
      "Epoch 11 | Training loss = 6.683049360911052 | Test loss = 7.457426230112712\n",
      "Epoch 12 | Training loss = 6.458459814389546 | Test loss = 7.25160566965739\n",
      "Epoch 13 | Training loss = 6.464967250823975 | Test loss = 7.268611749013265\n",
      "Epoch 14 | Training loss = 6.30130660533905 | Test loss = 7.445724646250407\n",
      "Epoch 15 | Training loss = 6.152454257011414 | Test loss = 7.303316752115886\n",
      "Epoch 16 | Training loss = 6.110154191652934 | Test loss = 7.768831253051758\n",
      "Epoch 17 | Training loss = 6.4737246831258135 | Test loss = 7.305837472279866\n",
      "Epoch 18 | Training loss = 6.130642652511597 | Test loss = 7.260700066884358\n",
      "Epoch 19 | Training loss = 6.1795047124226885 | Test loss = 7.752310434977214\n",
      "Epoch 20 | Training loss = 6.847569505373637 | Test loss = 7.2375203768412275\n",
      "Epoch 21 | Training loss = 5.849498271942139 | Test loss = 7.426011403401692\n",
      "Epoch 22 | Training loss = 5.689328630765279 | Test loss = 7.360159556070964\n",
      "Epoch 23 | Training loss = 5.919217030207316 | Test loss = 7.557902177174886\n",
      "Epoch 24 | Training loss = 5.595247864723206 | Test loss = 7.712940692901611\n",
      "Epoch 25 | Training loss = 5.99410621325175 | Test loss = 7.473759174346924\n",
      "Epoch 26 | Training loss = 5.695220788319905 | Test loss = 7.374248663584392\n",
      "Epoch 27 | Training loss = 5.233346859614055 | Test loss = 8.716233889261881\n",
      "Epoch 28 | Training loss = 5.311874866485596 | Test loss = 7.460696538289388\n",
      "Epoch 29 | Training loss = 5.170274357000987 | Test loss = 7.367470900217692\n",
      "Epoch 30 | Training loss = 5.277544101079305 | Test loss = 7.723796526590983\n",
      "Epoch 31 | Training loss = 5.158679564793904 | Test loss = 8.18914794921875\n",
      "Epoch 32 | Training loss = 5.001008450984955 | Test loss = 8.004949887593588\n",
      "Epoch 33 | Training loss = 4.936037659645081 | Test loss = 8.164503892262777\n",
      "Epoch 34 | Training loss = 5.0555481513341265 | Test loss = 8.054234186808268\n",
      "Epoch 35 | Training loss = 4.989664196968079 | Test loss = 7.545074303944905\n",
      "Epoch 36 | Training loss = 5.0223904848098755 | Test loss = 10.338213602701822\n",
      "Epoch 37 | Training loss = 4.88206418355306 | Test loss = 7.399239857991536\n",
      "Epoch 38 | Training loss = 4.775976479053497 | Test loss = 7.338545958201091\n",
      "Epoch 39 | Training loss = 4.605921665827434 | Test loss = 9.316657702128092\n",
      "Epoch 40 | Training loss = 4.987745384375255 | Test loss = 8.820995807647705\n",
      "Epoch 41 | Training loss = 5.344822406768799 | Test loss = 8.110235214233398\n",
      "Epoch 42 | Training loss = 4.908206462860107 | Test loss = 7.359465916951497\n",
      "Epoch 43 | Training loss = 4.442914545536041 | Test loss = 9.370166142781576\n",
      "Epoch 44 | Training loss = 4.790550569693248 | Test loss = 9.435531934102377\n",
      "Epoch 45 | Training loss = 5.084100663661957 | Test loss = 7.572587649027507\n",
      "Epoch 46 | Training loss = 4.663962960243225 | Test loss = 7.471934000651042\n",
      "Epoch 47 | Training loss = 4.2054354548454285 | Test loss = 8.533408959706625\n",
      "Epoch 48 | Training loss = 4.906412780284882 | Test loss = 7.557425816853841\n",
      "Epoch 49 | Training loss = 4.204350590705872 | Test loss = 7.616273880004883\n",
      "Epoch 50 | Training loss = 3.954305648803711 | Test loss = 7.580882390340169\n",
      "Epoch 51 | Training loss = 3.839664657910665 | Test loss = 6.623025099436442\n",
      "Epoch 52 | Training loss = 4.088016192118327 | Test loss = 7.906350135803223\n",
      "Epoch 53 | Training loss = 3.661145508289337 | Test loss = 7.11151107152303\n",
      "Epoch 54 | Training loss = 4.075277705987294 | Test loss = 7.276913642883301\n",
      "Epoch 55 | Training loss = 4.524488925933838 | Test loss = 8.647719065348307\n",
      "Epoch 56 | Training loss = 3.9496471683184304 | Test loss = 7.062202612559001\n",
      "Epoch 57 | Training loss = 3.781195819377899 | Test loss = 7.538710435231526\n",
      "Epoch 58 | Training loss = 3.7312841614087424 | Test loss = 7.409555753072103\n",
      "Epoch 59 | Training loss = 3.542662580808004 | Test loss = 6.8000820477803545\n",
      "Epoch 60 | Training loss = 3.3963104486465454 | Test loss = 7.255311806996663\n",
      "Epoch 61 | Training loss = 3.715904871622721 | Test loss = 7.670571327209473\n",
      "Epoch 62 | Training loss = 3.603720486164093 | Test loss = 7.174979209899902\n",
      "Epoch 63 | Training loss = 3.111020008722941 | Test loss = 7.517507712046306\n",
      "Epoch 64 | Training loss = 2.9927901526292167 | Test loss = 6.956284046173096\n",
      "Epoch 65 | Training loss = 3.104426602522532 | Test loss = 6.657466729482015\n",
      "Epoch 66 | Training loss = 3.1579800645510354 | Test loss = 7.361992835998535\n",
      "Epoch 67 | Training loss = 3.1950008273124695 | Test loss = 7.615543524424235\n",
      "Epoch 68 | Training loss = 3.2167858282725015 | Test loss = 6.9711306889851885\n",
      "Epoch 69 | Training loss = 2.873818119366964 | Test loss = 7.59152094523112\n",
      "Epoch 70 | Training loss = 2.791612148284912 | Test loss = 6.757821877797444\n",
      "Epoch 71 | Training loss = 2.372285137573878 | Test loss = 7.209214210510254\n",
      "Epoch 72 | Training loss = 2.2050035496552787 | Test loss = 6.301183223724365\n",
      "Epoch 73 | Training loss = 2.233709752559662 | Test loss = 6.9727522532145185\n",
      "Epoch 74 | Training loss = 2.0212566455205283 | Test loss = 6.684305985768636\n",
      "Epoch 75 | Training loss = 1.9157992204030354 | Test loss = 6.8166273434956866\n",
      "Epoch 76 | Training loss = 1.9180757304032643 | Test loss = 6.472425619761149\n",
      "Epoch 77 | Training loss = 1.9207812746365864 | Test loss = 7.313731511433919\n",
      "Epoch 78 | Training loss = 1.8641972442468007 | Test loss = 6.676768779754639\n",
      "Epoch 79 | Training loss = 1.8730249007542927 | Test loss = 6.524538675944011\n",
      "Epoch 80 | Training loss = 1.906863381465276 | Test loss = 6.554830074310303\n",
      "Epoch 81 | Training loss = 1.8107316096623738 | Test loss = 6.803676764170329\n",
      "Epoch 82 | Training loss = 1.9679962297280629 | Test loss = 6.918850739796956\n",
      "Epoch 83 | Training loss = 2.173809379339218 | Test loss = 6.723787307739258\n",
      "Epoch 84 | Training loss = 2.12211004892985 | Test loss = 6.382217089335124\n",
      "Epoch 85 | Training loss = 1.8599537710348766 | Test loss = 6.435429732004802\n",
      "Epoch 86 | Training loss = 1.7688421209653218 | Test loss = 7.050688902537028\n",
      "Epoch 87 | Training loss = 1.5587200423081715 | Test loss = 6.624324957529704\n",
      "Epoch 88 | Training loss = 1.793139785528183 | Test loss = 6.634120623270671\n",
      "Epoch 89 | Training loss = 1.580073356628418 | Test loss = 6.165364583333333\n",
      "Epoch 90 | Training loss = 1.593991219997406 | Test loss = 6.74043067296346\n",
      "Epoch 91 | Training loss = 1.5151897966861725 | Test loss = 6.146823724110921\n",
      "Epoch 92 | Training loss = 1.251472810904185 | Test loss = 6.237233479817708\n",
      "Epoch 93 | Training loss = 1.1520024140675862 | Test loss = 6.47971773147583\n",
      "Epoch 94 | Training loss = 1.0333110143740971 | Test loss = 6.291062513987224\n",
      "Epoch 95 | Training loss = 0.9740281850099564 | Test loss = 6.414148489634196\n",
      "Epoch 96 | Training loss = 0.9260571599006653 | Test loss = 6.394530773162842\n",
      "Epoch 97 | Training loss = 0.9330471356709799 | Test loss = 6.714316368103027\n",
      "Epoch 98 | Training loss = 0.8952493568261465 | Test loss = 6.549920399983724\n",
      "Epoch 99 | Training loss = 0.9173599779605865 | Test loss = 7.178652127583821\n",
      "Epoch 100 | Training loss = 1.1091475288073223 | Test loss = 6.294843355814616\n",
      "Epoch 101 | Training loss = 1.1741957763830821 | Test loss = 6.575557390848796\n",
      "Epoch 102 | Training loss = 1.0923271576563518 | Test loss = 6.9102678298950195\n",
      "Epoch 103 | Training loss = 1.0472375005483627 | Test loss = 7.178457101186116\n",
      "Epoch 104 | Training loss = 1.0187384883562725 | Test loss = 7.474627176920573\n",
      "Epoch 105 | Training loss = 1.0321610470612843 | Test loss = 6.921696027119954\n",
      "Epoch 106 | Training loss = 1.0490152339140575 | Test loss = 6.855166435241699\n",
      "Epoch 107 | Training loss = 0.8121306548515955 | Test loss = 7.380887985229492\n",
      "Epoch 108 | Training loss = 0.7228259742259979 | Test loss = 7.1546346346537275\n",
      "Epoch 109 | Training loss = 0.7203007414937019 | Test loss = 6.783588886260986\n",
      "Epoch 110 | Training loss = 0.7144513378540674 | Test loss = 7.257813453674316\n",
      "Epoch 111 | Training loss = 0.7707619170347849 | Test loss = 7.534518718719482\n",
      "Epoch 112 | Training loss = 0.7008016208807627 | Test loss = 6.92697811126709\n",
      "Epoch 113 | Training loss = 0.698832243680954 | Test loss = 6.877681573232015\n",
      "Epoch 114 | Training loss = 0.6262379884719849 | Test loss = 6.709219137827556\n",
      "Epoch 115 | Training loss = 0.6228987301389376 | Test loss = 7.0575839678446455\n",
      "Epoch 116 | Training loss = 0.5377280935645103 | Test loss = 6.909870147705078\n",
      "Epoch 117 | Training loss = 0.6102405786514282 | Test loss = 7.29032834370931\n",
      "Epoch 118 | Training loss = 0.7718735088904699 | Test loss = 6.593265374501546\n",
      "Epoch 119 | Training loss = 0.8773818810780843 | Test loss = 7.51484219233195\n",
      "Epoch 120 | Training loss = 0.9128710428873698 | Test loss = 8.046642621358236\n",
      "Epoch 121 | Training loss = 1.092475359638532 | Test loss = 7.3825985590616865\n",
      "Epoch 122 | Training loss = 0.9922620356082916 | Test loss = 7.538677056630452\n",
      "Epoch 123 | Training loss = 1.0691945801178615 | Test loss = 6.604852358500163\n",
      "Epoch 124 | Training loss = 0.9069714347521464 | Test loss = 6.720730781555176\n",
      "Epoch 125 | Training loss = 0.66917618115743 | Test loss = 6.817721048990886\n",
      "Epoch 126 | Training loss = 0.5873821998635927 | Test loss = 6.740733782450358\n",
      "Epoch 127 | Training loss = 0.539743202428023 | Test loss = 6.672304789225261\n",
      "Epoch 128 | Training loss = 0.5039287482698759 | Test loss = 6.830625693003337\n",
      "Epoch 129 | Training loss = 0.5082085455457369 | Test loss = 6.830155531565349\n",
      "Epoch 130 | Training loss = 0.4724198505282402 | Test loss = 7.234880129496257\n",
      "Epoch 131 | Training loss = 0.4105052227775256 | Test loss = 7.2624867757161455\n",
      "Epoch 132 | Training loss = 0.41156882296005887 | Test loss = 7.11292839050293\n",
      "Epoch 133 | Training loss = 0.35764410843451816 | Test loss = 7.318144798278809\n",
      "Epoch 134 | Training loss = 0.3267329881588618 | Test loss = 7.227935632069905\n",
      "Epoch 135 | Training loss = 0.31462038060029346 | Test loss = 7.292377312978108\n",
      "Epoch 136 | Training loss = 0.28268861894806224 | Test loss = 7.221166133880615\n",
      "Epoch 137 | Training loss = 0.300169928620259 | Test loss = 6.827915986378987\n",
      "Epoch 138 | Training loss = 0.3357594795525074 | Test loss = 7.058965841929118\n",
      "Epoch 139 | Training loss = 0.34621572121977806 | Test loss = 6.82558806737264\n",
      "Epoch 140 | Training loss = 0.30075127879778546 | Test loss = 6.792190392812093\n",
      "Epoch 141 | Training loss = 0.30225124085942906 | Test loss = 6.955323219299316\n",
      "Epoch 142 | Training loss = 0.31144343689084053 | Test loss = 7.158711592356364\n",
      "Epoch 143 | Training loss = 0.25827523941795033 | Test loss = 6.98145850499471\n",
      "Epoch 144 | Training loss = 0.23805713777740797 | Test loss = 7.020533084869385\n",
      "Epoch 145 | Training loss = 0.22542567178606987 | Test loss = 6.856029669443767\n",
      "Epoch 146 | Training loss = 0.22970941041906676 | Test loss = 6.658161004384358\n",
      "Epoch 147 | Training loss = 0.23467704529563585 | Test loss = 6.917084693908691\n",
      "Epoch 148 | Training loss = 0.2029310849805673 | Test loss = 6.899237950642903\n",
      "Epoch 149 | Training loss = 0.1891473134358724 | Test loss = 6.774801095326741\n",
      "Epoch 150 | Training loss = 0.19283535207311311 | Test loss = 6.8686269124348955\n",
      "Epoch 151 | Training loss = 0.15915845210353533 | Test loss = 7.119311014811198\n",
      "Epoch 152 | Training loss = 0.13957157855232558 | Test loss = 6.859864711761475\n",
      "Epoch 153 | Training loss = 0.13562305209537348 | Test loss = 7.1136447588602705\n",
      "Epoch 154 | Training loss = 0.1325079072266817 | Test loss = 7.1050489743550616\n",
      "Epoch 155 | Training loss = 0.1471351763854424 | Test loss = 6.935640970865886\n",
      "Epoch 156 | Training loss = 0.16420554369688034 | Test loss = 7.08268674214681\n",
      "Epoch 157 | Training loss = 0.15856567149360976 | Test loss = 7.4606679280598955\n",
      "Epoch 158 | Training loss = 0.1633571001390616 | Test loss = 7.264059066772461\n",
      "Epoch 159 | Training loss = 0.15617412204543749 | Test loss = 7.116210460662842\n",
      "Epoch 160 | Training loss = 0.1490945170323054 | Test loss = 6.950680255889893\n",
      "Epoch 161 | Training loss = 0.13986790490647158 | Test loss = 7.141725540161133\n",
      "Epoch 162 | Training loss = 0.13585501536726952 | Test loss = 7.150291760762532\n",
      "Epoch 163 | Training loss = 0.1389093908170859 | Test loss = 7.06283966700236\n",
      "Epoch 164 | Training loss = 0.15456417202949524 | Test loss = 7.290194511413574\n",
      "Epoch 165 | Training loss = 0.13481178507208824 | Test loss = 7.210611820220947\n",
      "Epoch 166 | Training loss = 0.11924214040239652 | Test loss = 6.966803073883057\n",
      "Epoch 167 | Training loss = 0.11092117490867774 | Test loss = 7.065706253051758\n",
      "Epoch 168 | Training loss = 0.08993036424120267 | Test loss = 6.942323207855225\n",
      "Epoch 169 | Training loss = 0.08866229405005772 | Test loss = 6.989230632781982\n",
      "Epoch 170 | Training loss = 0.08897398598492146 | Test loss = 7.03932539621989\n",
      "Epoch 171 | Training loss = 0.08690561323116223 | Test loss = 7.054979960123698\n",
      "Epoch 172 | Training loss = 0.10389360785484314 | Test loss = 7.193077723185222\n",
      "Epoch 173 | Training loss = 0.10645211301743984 | Test loss = 7.0210920969645185\n",
      "Epoch 174 | Training loss = 0.10000014243026574 | Test loss = 7.229837894439697\n",
      "Epoch 175 | Training loss = 0.08499792777001858 | Test loss = 7.132473468780518\n",
      "Epoch 176 | Training loss = 0.10572813761730988 | Test loss = 7.229486465454102\n",
      "Epoch 177 | Training loss = 0.08473662938922644 | Test loss = 7.098682562510173\n",
      "Epoch 178 | Training loss = 0.08452883673210938 | Test loss = 7.1018673578898115\n",
      "Epoch 179 | Training loss = 0.08665058699746926 | Test loss = 7.179813543955485\n",
      "Epoch 180 | Training loss = 0.07087407726794481 | Test loss = 7.177820523579915\n",
      "Epoch 181 | Training loss = 0.08102552872151136 | Test loss = 6.947853088378906\n",
      "Epoch 182 | Training loss = 0.0763509205232064 | Test loss = 7.043711344401042\n",
      "Epoch 183 | Training loss = 0.07580559700727463 | Test loss = 7.052907784779866\n",
      "Epoch 184 | Training loss = 0.07374276344974835 | Test loss = 7.270150661468506\n",
      "Epoch 185 | Training loss = 0.07077554520219564 | Test loss = 7.012639999389648\n",
      "Epoch 186 | Training loss = 0.0705422442406416 | Test loss = 7.102217594782512\n",
      "Epoch 187 | Training loss = 0.07959813810884953 | Test loss = 7.0220615069071455\n",
      "Epoch 188 | Training loss = 0.06781214475631714 | Test loss = 7.22783088684082\n",
      "Epoch 189 | Training loss = 0.05674735177308321 | Test loss = 6.939308961232503\n",
      "Epoch 190 | Training loss = 0.05296306560436884 | Test loss = 7.0636115074157715\n",
      "Epoch 191 | Training loss = 0.050740872820218406 | Test loss = 7.07081667582194\n",
      "Epoch 192 | Training loss = 0.051322145853191614 | Test loss = 7.117046356201172\n",
      "Epoch 193 | Training loss = 0.05807175223405162 | Test loss = 7.12308915456136\n",
      "Epoch 194 | Training loss = 0.059964981861412525 | Test loss = 6.979617118835449\n",
      "Epoch 195 | Training loss = 0.06058987850944201 | Test loss = 7.163967768351237\n",
      "Epoch 196 | Training loss = 0.06538106594234705 | Test loss = 7.074053605397542\n",
      "Epoch 197 | Training loss = 0.06367954735954602 | Test loss = 7.050074736277263\n",
      "Epoch 198 | Training loss = 0.06859548979749282 | Test loss = 7.172634760538737\n",
      "Epoch 199 | Training loss = 0.0658837417140603 | Test loss = 7.4129378000895185\n",
      "Epoch 200 | Training loss = 0.06711154617369175 | Test loss = 7.351679801940918\n",
      "Epoch 201 | Training loss = 0.05937778825561205 | Test loss = 7.214803377787272\n",
      "Epoch 202 | Training loss = 0.053437541549404464 | Test loss = 7.169002374013265\n",
      "Epoch 203 | Training loss = 0.05010673931489388 | Test loss = 7.192986488342285\n",
      "Epoch 204 | Training loss = 0.05682837528487047 | Test loss = 7.045999685923259\n",
      "Epoch 205 | Training loss = 0.06369465837876002 | Test loss = 7.02634843190511\n",
      "Epoch 206 | Training loss = 0.06479415421684583 | Test loss = 7.262862046559651\n",
      "Epoch 207 | Training loss = 0.05752301619698604 | Test loss = 7.058355172475179\n",
      "Epoch 208 | Training loss = 0.056749578254918255 | Test loss = 7.1183851559956866\n",
      "Epoch 209 | Training loss = 0.03924915458386143 | Test loss = 7.0723347663879395\n",
      "Epoch 210 | Training loss = 0.04486606844390432 | Test loss = 7.16710090637207\n",
      "Epoch 211 | Training loss = 0.043209942212949194 | Test loss = 7.265857537587483\n",
      "Epoch 212 | Training loss = 0.03764419754346212 | Test loss = 7.017830053965251\n",
      "Epoch 213 | Training loss = 0.03830642020329833 | Test loss = 7.2828049659729\n",
      "Epoch 214 | Training loss = 0.040013391679773726 | Test loss = 7.127889474232991\n",
      "Epoch 215 | Training loss = 0.056453726875285305 | Test loss = 7.110540231068929\n",
      "Epoch 216 | Training loss = 0.05054537175844113 | Test loss = 6.995007832845052\n",
      "Epoch 217 | Training loss = 0.05187288594121734 | Test loss = 7.0572249094645185\n",
      "Epoch 218 | Training loss = 0.045240010445316635 | Test loss = 7.190978209177653\n",
      "Epoch 219 | Training loss = 0.04440819487596551 | Test loss = 7.136943340301514\n",
      "Epoch 220 | Training loss = 0.04152127898608645 | Test loss = 7.220896561940511\n",
      "Epoch 221 | Training loss = 0.04299829558779796 | Test loss = 7.035225550333659\n",
      "Epoch 222 | Training loss = 0.052984040696173906 | Test loss = 7.135464032491048\n",
      "Epoch 223 | Training loss = 0.04975450628747543 | Test loss = 6.9706471761067705\n",
      "Epoch 224 | Training loss = 0.03839198484395941 | Test loss = 7.083501974741618\n",
      "Epoch 225 | Training loss = 0.03780347822854916 | Test loss = 7.026379267374675\n",
      "Epoch 226 | Training loss = 0.04286686067158977 | Test loss = 7.22188154856364\n",
      "Epoch 227 | Training loss = 0.04476484966774782 | Test loss = 6.974689642588298\n",
      "Epoch 228 | Training loss = 0.04337579058483243 | Test loss = 7.18742020924886\n",
      "Epoch 229 | Training loss = 0.04201626249899467 | Test loss = 7.015380382537842\n",
      "Epoch 230 | Training loss = 0.038240408059209585 | Test loss = 7.141896883646647\n",
      "Epoch 231 | Training loss = 0.03197534801438451 | Test loss = 7.112578709920247\n",
      "Epoch 232 | Training loss = 0.03263139522944888 | Test loss = 7.150844256083171\n",
      "Epoch 233 | Training loss = 0.039175561318794884 | Test loss = 7.1582614580790205\n",
      "Epoch 234 | Training loss = 0.040844222685943045 | Test loss = 7.061305681864421\n",
      "Epoch 235 | Training loss = 0.03846924457078179 | Test loss = 7.061006228129069\n",
      "Epoch 236 | Training loss = 0.03743943711742759 | Test loss = 7.012125492095947\n",
      "Epoch 237 | Training loss = 0.03294256251926223 | Test loss = 7.178886254628499\n",
      "Epoch 238 | Training loss = 0.032561151310801506 | Test loss = 6.904459476470947\n",
      "Epoch 239 | Training loss = 0.03374546145399412 | Test loss = 7.113409519195557\n",
      "Epoch 240 | Training loss = 0.0290449649716417 | Test loss = 7.172322750091553\n",
      "Epoch 241 | Training loss = 0.02853616156304876 | Test loss = 7.14370584487915\n",
      "Epoch 242 | Training loss = 0.030012745410203934 | Test loss = 7.045506318410237\n",
      "Epoch 243 | Training loss = 0.03134457782531778 | Test loss = 7.1051375071207685\n",
      "Epoch 244 | Training loss = 0.03886249801144004 | Test loss = 7.1259206136067705\n",
      "Epoch 245 | Training loss = 0.03237491069982449 | Test loss = 6.9052019119262695\n",
      "Epoch 246 | Training loss = 0.024946696124970913 | Test loss = 7.126733620961507\n",
      "Epoch 247 | Training loss = 0.02448397238428394 | Test loss = 7.166199207305908\n",
      "Epoch 248 | Training loss = 0.03267404615568618 | Test loss = 7.1941938400268555\n",
      "Epoch 249 | Training loss = 0.03584619533891479 | Test loss = 7.245166778564453\n",
      "Epoch 250 | Training loss = 0.03793710966904958 | Test loss = 7.0573039054870605\n",
      "Epoch 251 | Training loss = 0.041407476334522166 | Test loss = 7.174035390218099\n",
      "Epoch 252 | Training loss = 0.03610601477945844 | Test loss = 7.136183579762776\n",
      "Epoch 253 | Training loss = 0.02852101461030543 | Test loss = 6.99583101272583\n",
      "Epoch 254 | Training loss = 0.027258117062350113 | Test loss = 7.055797576904297\n",
      "Epoch 255 | Training loss = 0.02336345985531807 | Test loss = 7.075816790262858\n",
      "Epoch 256 | Training loss = 0.028320728180309136 | Test loss = 7.087850888570149\n",
      "Epoch 257 | Training loss = 0.030746298531691234 | Test loss = 7.05655574798584\n",
      "Epoch 258 | Training loss = 0.022148288087919354 | Test loss = 6.973833878835042\n",
      "Epoch 259 | Training loss = 0.03137577247495452 | Test loss = 6.974117755889893\n",
      "Epoch 260 | Training loss = 0.03214957828943928 | Test loss = 7.149989604949951\n",
      "Epoch 261 | Training loss = 0.025851914814362924 | Test loss = 7.05978790918986\n",
      "Epoch 262 | Training loss = 0.040402988359952964 | Test loss = 7.102757453918457\n",
      "Epoch 263 | Training loss = 0.03248435336475571 | Test loss = 7.114956537882487\n",
      "Epoch 264 | Training loss = 0.02644244208931923 | Test loss = 7.143142382303874\n",
      "Epoch 265 | Training loss = 0.027999113003412884 | Test loss = 7.092744827270508\n",
      "Epoch 266 | Training loss = 0.025656337694575388 | Test loss = 7.080808957417806\n",
      "Epoch 267 | Training loss = 0.020515841897577047 | Test loss = 7.135305245717366\n",
      "Epoch 268 | Training loss = 0.019451366349433858 | Test loss = 7.120466391245524\n",
      "Epoch 269 | Training loss = 0.022874997463077307 | Test loss = 7.039902210235596\n",
      "Epoch 270 | Training loss = 0.020476975788672764 | Test loss = 7.135878562927246\n",
      "Epoch 271 | Training loss = 0.02375153739315768 | Test loss = 7.028931299845378\n",
      "Epoch 272 | Training loss = 0.02312899986281991 | Test loss = 6.9943421681722\n",
      "Epoch 273 | Training loss = 0.023270050063729286 | Test loss = 7.018632888793945\n",
      "Epoch 274 | Training loss = 0.026355910425384838 | Test loss = 7.138699213663737\n",
      "Epoch 275 | Training loss = 0.02635193585107724 | Test loss = 7.176530361175537\n",
      "Epoch 276 | Training loss = 0.023457713114718597 | Test loss = 7.142026901245117\n",
      "Epoch 277 | Training loss = 0.026892674155533314 | Test loss = 7.076443672180176\n",
      "Epoch 278 | Training loss = 0.027368701218316954 | Test loss = 7.149941126505534\n",
      "Epoch 279 | Training loss = 0.026090296373392146 | Test loss = 6.964518388112386\n",
      "Epoch 280 | Training loss = 0.021191099658608437 | Test loss = 7.098592599232991\n",
      "Epoch 281 | Training loss = 0.021654841878140967 | Test loss = 7.075358231862386\n",
      "Epoch 282 | Training loss = 0.023272494940708082 | Test loss = 7.182628631591797\n",
      "Epoch 283 | Training loss = 0.02016164897941053 | Test loss = 7.060605049133301\n",
      "Epoch 284 | Training loss = 0.020315467768038314 | Test loss = 7.145565986633301\n",
      "Epoch 285 | Training loss = 0.021439096424728632 | Test loss = 7.067168712615967\n",
      "Epoch 286 | Training loss = 0.018919993735228974 | Test loss = 7.0868252118428545\n",
      "Epoch 287 | Training loss = 0.016336297383531928 | Test loss = 6.925116062164307\n",
      "Epoch 288 | Training loss = 0.018329204603408773 | Test loss = 7.0246842702229815\n",
      "Epoch 289 | Training loss = 0.017204065341502428 | Test loss = 6.94478193918864\n",
      "Epoch 290 | Training loss = 0.023372555850073695 | Test loss = 7.056288083394368\n",
      "Epoch 291 | Training loss = 0.024874796935667593 | Test loss = 7.069322268168132\n",
      "Epoch 292 | Training loss = 0.02148261076460282 | Test loss = 7.040146509806315\n",
      "Epoch 293 | Training loss = 0.02720534064186116 | Test loss = 7.069794813791911\n",
      "Epoch 294 | Training loss = 0.02179709170013666 | Test loss = 7.103450457255046\n",
      "Epoch 295 | Training loss = 0.02445947037388881 | Test loss = 7.009158134460449\n",
      "Epoch 296 | Training loss = 0.02586558337012927 | Test loss = 7.08466100692749\n",
      "Epoch 297 | Training loss = 0.02460027765482664 | Test loss = 7.054393291473389\n",
      "Epoch 298 | Training loss = 0.0224605902719001 | Test loss = 7.186757246653239\n",
      "Epoch 299 | Training loss = 0.02390672025891642 | Test loss = 6.95885435740153\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "loss_tracker = np.zeros((num_epochs, 2))\n",
    "\n",
    "num_train_batches = len(train_loader)\n",
    "num_test_batches = len(test_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    for batch_idx, (ft, lbl) in enumerate(train_loader):\n",
    "        ft, lbl = ft.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(ft)\n",
    "        loss = crit(output, lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * ft.shape[0]\n",
    "    train_loss /= len(train_dataset)\n",
    "    loss_tracker[epoch, 0] = train_loss\n",
    "        \n",
    "        \n",
    "    model = model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ft, lbl) in enumerate(test_loader):\n",
    "            ft, lbl = ft.to(device), lbl.to(device)\n",
    "            output = model(ft)\n",
    "            loss = crit(output, lbl)\n",
    "            test_loss += loss.item() * ft.shape[0]\n",
    "    test_loss /= len(test_dataset)\n",
    "    loss_tracker[epoch, 1] = test_loss\n",
    "            \n",
    "    print('Epoch {} | Training loss = {} | Test loss = {}'.format(epoch, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1262ce8",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "CNN seemed to help accuracy, as well as more linear layers. However, it is overfitting heavily. Batchnorm didn't really make a difference, dropout seems to make things worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f333c6-a3e6-491a-b71c-d9ddae7c8b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_dir = Path('./loss_plots')\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_tracker)\n",
    "plt.title('Training vs Testing Loss (Mean Loss Per Batch)')\n",
    "plt.legend(['Training loss', 'Test loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(img_dir / 'sq_eucloss_with_batchnorm_and_conv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c95018b-5f35-43fb-96b4-4289d5078dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_dir = Path('./models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), model_dir / 'sq_eucloss_with_batchnorm_and_conv.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b856b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
