{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76534fa3-6d11-429d-af28-5978ea704bb2",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "1. [x] load dataset into tensor, convert to float32  \n",
    "    - [x] apply normalization\n",
    "2. [x] Implement DataParallel training  \n",
    "    - [x] increase minibatch size to 128 for 32 per device\n",
    "3. [ ] try training and benchmark speed\n",
    "4. [ ] fix simulation script to get the correct labels and retrain\n",
    "\n",
    "gpu datasheet (we have sxm version): https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf\n",
    "\n",
    "TODO: change dataparallel to distributed data parallel at some point, and move everything from the notebook into a training script\n",
    "\n",
    "Links:\n",
    "https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae07a7-9241-40fe-bd45-013bd062aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from torchvision.transforms import Normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a49077-d5dc-49b9-a495-8ebeb8ea5b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82343b3f-3318-486b-ad5a-5dfc12dd5e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34698c-86c9-4488-9894-87f468a79068",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "    \n",
    "# use devices 0-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996643e-08b9-4441-9791-9e29639b0dc7",
   "metadata": {},
   "source": [
    "#### Load matlab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "597f8402-11a0-47c8-9e97-33cb19abc744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42581, 1, 16, 48), dtype('float32'))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feats = h5py.File('samplesChirp.mat', 'r')\n",
    "# labels = h5py.File('labelsChirp.mat', 'r')\n",
    "feats, labels = scipy.io.loadmat('output/feats4T_.1R.mat'), scipy.io.loadmat('output/labels4T_.1R.mat')\n",
    "feats = feats['features']\n",
    "labels = labels['lp']\n",
    "\n",
    "labels = labels.astype('float32')\n",
    "labels = labels.T\n",
    "\n",
    "feats = feats.astype('float32')\n",
    "feats = feats.T\n",
    "feats = feats.reshape((feats.shape[0], -1, feats.shape[-1]))\n",
    "feats = feats[:, None, :, :]\n",
    "feats.shape, feats.dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d497c-88fb-43e4-8fac-42d00a432ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c3ace-fb10-478a-9f34-a8884e1b5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats['samples'], labels['labels']['position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8634d3b-6d1d-4dce-86d4-94f3227eadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats_da = da.from_array(feats['samples']).astype('float32') # cast to float32\n",
    "# feats_da = feats_da[:,None,:,:] # add channel dimension\n",
    "# feats_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5231476-47b0-4ee0-a09f-68e231d6b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_da = da.from_array(labels['labels']['position']).astype('float32')\n",
    "# labels_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3a1e4d4-a8a1-4d69-bde3-3c85148cd1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([42581, 1, 16, 48]),\n",
       " torch.Size([42581, 3]),\n",
       " torch.float32,\n",
       " torch.float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor(feats)\n",
    "Y = torch.Tensor(labels)\n",
    "X.shape, Y.shape, X.dtype, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9c52784-f7ed-4a90-b50c-9765b496a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cee4f9-70bd-4c6d-87fe-5e3529c9ee44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c166c57-66a6-4cf4-b60c-cd8f02182a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0168), tensor(0.0659))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "X_train_mean, X_train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824ac67-f5e8-4c72-9e82-cb260a15c702",
   "metadata": {},
   "source": [
    "NORMALIZATION:\n",
    "1. create custom Dataset class based on TensorDataset that will apply a normalization transform if provided\n",
    "2. create train and test datasets, pass in X_train_mean and X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4322f908-18b0-4d67-8fa2-3a419cc8b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, tensors, transforms=None):\n",
    "        # check to make sure number of samples match\n",
    "        assert all(tensors[0].shape[0] == tens.shape[0] for tens in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            x = self.transforms(x)\n",
    "            \n",
    "        y = self.tensors[1][index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.tensors[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c769512-f662-4489-9c87-781921c41fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.CustomTensorDataset at 0x7fefd056a820>,\n",
       " <__main__.CustomTensorDataset at 0x7fefd056a910>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CustomTensorDataset([X_train, Y_train], Normalize(X_train_mean, X_train_std))\n",
    "test_dataset = CustomTensorDataset([X_test, Y_test], Normalize(X_train_mean, X_train_std))\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033ff87-8f4e-4f76-9e95-ab52566b8859",
   "metadata": {},
   "source": [
    "### sample shape for spectrogram dataset: (minibatch_size, 1, 300, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a41f318-1a60-49df-aa03-0e3a70fcccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fefd03fb9a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fefd03fbee0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65c698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e06d19fa-4f93-4466-9439-b085664b0896",
   "metadata": {},
   "source": [
    "### Define models and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03cf0009-b224-4b40-a244-5803a0cef500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Dropout2d(p=0.2),\n",
    "        )\n",
    "        # linear_in_dim = int(300/2/2/2*1024/2/2/2*64)\n",
    "        linear_in_dim = 768\n",
    "        self.linear1 = nn.Linear(linear_in_dim, 500)\n",
    "        # self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.linear2 = nn.Linear(500, 100)\n",
    "        self.linear2_2 = nn.Linear(100, 20)\n",
    "        # self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.linear3 = nn.Linear(20, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.seq(x)\n",
    "        out = out.view(out.size(0), -1) # flatten to (batch size, int)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        # out = self.dropout1(out)\n",
    "        out = F.relu(self.linear2(out))\n",
    "        out = F.relu(self.linear2_2(out))\n",
    "        # out = self.dropout2(out)\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "        \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(768, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "def EucLoss(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    assert a.shape == b.shape\n",
    "    assert b.shape[-1] == 3\n",
    "    return torch.sum((a-b).square(), dim=-1).sqrt().mean()\n",
    "\n",
    "def EucLossSquared(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    assert a.shape == b.shape\n",
    "    assert b.shape[-1] == 3\n",
    "    return torch.sum((a-b).square(), dim=-1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae4d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "348fcabd-f549-4bd5-8e8e-8a995acec81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MyCNN(\n",
       "    (seq): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (linear1): Linear(in_features=768, out_features=500, bias=True)\n",
       "    (linear2): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (linear2_2): Linear(in_features=100, out_features=20, bias=True)\n",
       "    (linear3): Linear(in_features=20, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = SimpleNN().to(device)\n",
    "\n",
    "model = nn.DataParallel(MyCNN(), device_ids=[0,1,2,3]).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd1d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45247b73-15e4-44a8-b4e8-e737dff343bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = EucLoss\n",
    "# crit = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855d290-d4e8-4a60-ad2f-c23c10af4117",
   "metadata": {},
   "source": [
    "#### Training/Evaluating NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6d8b7-884d-4d93-be50-8fe96ab82449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Training loss = 2.1579369687317347 | Test loss = 1.8354578472798988\n",
      "Epoch 1 | Training loss = 1.494756790661039 | Test loss = 1.3717727515707836\n",
      "Epoch 2 | Training loss = 1.2566582799909707 | Test loss = 1.2103155349584984\n",
      "Epoch 3 | Training loss = 1.1246605706696446 | Test loss = 1.1068601154505653\n",
      "Epoch 4 | Training loss = 1.0143610300061399 | Test loss = 0.975734796905596\n",
      "Epoch 5 | Training loss = 0.9683300960562273 | Test loss = 0.991350787261485\n",
      "Epoch 6 | Training loss = 0.9277311212415145 | Test loss = 0.9637288499063235\n",
      "Epoch 7 | Training loss = 0.8903996142606435 | Test loss = 0.9196464922753693\n",
      "Epoch 8 | Training loss = 0.8708204357430318 | Test loss = 1.1441906854917958\n",
      "Epoch 9 | Training loss = 0.8434846121249148 | Test loss = 0.8928457128479543\n",
      "Epoch 10 | Training loss = 0.8179104809696095 | Test loss = 0.885201906995537\n",
      "Epoch 11 | Training loss = 0.8067966478707478 | Test loss = 1.0056982117806632\n",
      "Epoch 12 | Training loss = 0.7949048688345641 | Test loss = 1.1827057345982546\n",
      "Epoch 13 | Training loss = 0.7704436272284748 | Test loss = 0.8158107926493675\n",
      "Epoch 14 | Training loss = 0.7546548138710746 | Test loss = 0.8287545170836624\n",
      "Epoch 15 | Training loss = 0.7370622275915992 | Test loss = 0.8662658172327266\n",
      "Epoch 16 | Training loss = 0.7229601031404641 | Test loss = 0.831328505807406\n",
      "Epoch 17 | Training loss = 0.7183571697513281 | Test loss = 0.8506913865355801\n",
      "Epoch 18 | Training loss = 0.7030276546684326 | Test loss = 0.7634450744348078\n",
      "Epoch 19 | Training loss = 0.6866159408850645 | Test loss = 0.7904688985229896\n",
      "Epoch 20 | Training loss = 0.6738021205769508 | Test loss = 0.8004137158631802\n",
      "Epoch 21 | Training loss = 0.6712291835618724 | Test loss = 0.7948274985436922\n",
      "Epoch 22 | Training loss = 0.659070105830399 | Test loss = 0.7535746265804179\n",
      "Epoch 23 | Training loss = 0.6513148709438731 | Test loss = 0.7624147291248976\n",
      "Epoch 24 | Training loss = 0.6336649206391295 | Test loss = 0.775411573061854\n",
      "Epoch 25 | Training loss = 0.6299713364561268 | Test loss = 0.7968599746817082\n",
      "Epoch 26 | Training loss = 0.6200108093630825 | Test loss = 0.7521333470022061\n",
      "Epoch 27 | Training loss = 0.6146611629264799 | Test loss = 0.7377554246541465\n",
      "Epoch 28 | Training loss = 0.5969417522411606 | Test loss = 0.7587376006950973\n",
      "Epoch 29 | Training loss = 0.5949530610180394 | Test loss = 0.7369790754436368\n",
      "Epoch 30 | Training loss = 0.5834469451429476 | Test loss = 0.7331995530434725\n",
      "Epoch 31 | Training loss = 0.572887257322452 | Test loss = 0.7330651925919383\n",
      "Epoch 32 | Training loss = 0.5671713641407681 | Test loss = 0.7638559831942529\n",
      "Epoch 33 | Training loss = 0.5572985986299277 | Test loss = 0.7331000158270858\n",
      "Epoch 34 | Training loss = 0.5574171225640515 | Test loss = 0.7291068462148664\n",
      "Epoch 35 | Training loss = 0.5435162942809306 | Test loss = 0.7351490941716148\n",
      "Epoch 36 | Training loss = 0.5352838342040848 | Test loss = 0.7274916862761232\n",
      "Epoch 37 | Training loss = 0.5261231963765515 | Test loss = 0.7223435540329168\n",
      "Epoch 38 | Training loss = 0.5226979295991294 | Test loss = 0.7260873237307872\n",
      "Epoch 39 | Training loss = 0.5166259738776313 | Test loss = 0.7247665121099878\n",
      "Epoch 40 | Training loss = 0.5067272818721253 | Test loss = 0.7190087655417248\n",
      "Epoch 41 | Training loss = 0.4949631464486953 | Test loss = 0.7330547609742964\n",
      "Epoch 42 | Training loss = 0.49685600146602277 | Test loss = 0.7101898738668042\n",
      "Epoch 43 | Training loss = 0.48945146026382874 | Test loss = 0.7213465026094836\n",
      "Epoch 44 | Training loss = 0.4827139326640282 | Test loss = 0.7082913337955316\n",
      "Epoch 45 | Training loss = 0.47195905964942314 | Test loss = 0.7295634235114689\n",
      "Epoch 46 | Training loss = 0.46891344430694115 | Test loss = 0.7167968265436703\n",
      "Epoch 47 | Training loss = 0.46573674328845677 | Test loss = 0.7501594841067355\n",
      "Epoch 48 | Training loss = 0.4592294109825127 | Test loss = 0.7242148412537852\n",
      "Epoch 49 | Training loss = 0.4493918800757154 | Test loss = 0.7281469348815877\n",
      "Epoch 50 | Training loss = 0.4450100193341722 | Test loss = 0.7187503201029062\n",
      "Epoch 51 | Training loss = 0.43893888843289114 | Test loss = 0.7278373988284687\n",
      "Epoch 52 | Training loss = 0.43329333629110933 | Test loss = 0.7331731910085236\n",
      "Epoch 53 | Training loss = 0.4292141538150101 | Test loss = 0.7334887880709049\n",
      "Epoch 54 | Training loss = 0.42598099496343317 | Test loss = 0.7246160318947488\n",
      "Epoch 55 | Training loss = 0.41982662896137946 | Test loss = 0.7167052867421805\n",
      "Epoch 56 | Training loss = 0.40939552624945685 | Test loss = 0.7235336452145188\n",
      "Epoch 57 | Training loss = 0.4094528548450972 | Test loss = 0.7195276734385816\n",
      "Epoch 58 | Training loss = 0.4012571609233007 | Test loss = 0.7093827942944166\n",
      "Epoch 59 | Training loss = 0.39999175634446865 | Test loss = 0.7176897541323611\n",
      "Epoch 60 | Training loss = 0.39133263797533796 | Test loss = 0.7355723844610834\n",
      "Epoch 61 | Training loss = 0.3905770518848066 | Test loss = 0.7195199714925237\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500 \n",
    "loss_tracker = np.zeros((num_epochs, 2))\n",
    "\n",
    "num_train_batches = len(train_loader)\n",
    "num_test_batches = len(test_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    total_els = 0\n",
    "\n",
    "    \n",
    "    model = model.train()\n",
    "    \n",
    "    for batch_idx, (ft, lbl) in enumerate(train_loader):\n",
    "        # ft, lbl = ft.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(ft)\n",
    "        lbl = lbl.cuda()\n",
    "        loss = crit(output, lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * ft.shape[0]\n",
    "        total_els += ft.shape[0]\n",
    "    assert total_els == len(train_dataset)\n",
    "    train_loss /= len(train_dataset)\n",
    "    loss_tracker[epoch, 0] = train_loss\n",
    "        \n",
    "        \n",
    "    total_els = 0\n",
    "    model = model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (ft, lbl) in enumerate(test_loader):\n",
    "            # ft, lbl = ft.to(device), lbl.to(device)\n",
    "            output = model(ft)\n",
    "            lbl = lbl.cuda()\n",
    "            loss = crit(output, lbl)\n",
    "            test_loss += loss.item() * ft.shape[0] # multiply by number of samples in mini-batch to get total loss for batch\n",
    "            total_els += ft.shape[0]\n",
    "    assert total_els == len(test_dataset)\n",
    "    test_loss /= len(test_dataset) # get average loss per sample of whole dataset\n",
    "    loss_tracker[epoch, 1] = test_loss\n",
    "            \n",
    "    print('Epoch {} | Training loss = {} | Test loss = {}'.format(epoch, train_loss, test_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "img_dir = Path('./loss_plots')\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_tracker)\n",
    "plt.title('Training vs Testing Loss (Mean Loss Per Batch)')\n",
    "plt.legend(['Training loss', 'Test loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(img_dir / '04-12_spectrogram-dset_MyCNN-extended_euc-loss.png')\n",
    "\n",
    "# save model\n",
    "# model_dir = Path('./models')\n",
    "# model_dir.mkdir(parents=True, exist_ok=True)\n",
    "# torch.save(model.state_dict(), model_dir / 'spectrogram_dset_dgx.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1262ce8",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "CNN seemed to help accuracy, as well as more linear layers. However, it is overfitting heavily. Batchnorm didn't really make a difference, dropout seems to make things worse. try running on stampede\n",
    "\n",
    "\n",
    "#### Spectrogram dataset by itself doesn't perform well:\n",
    "\n",
    "Epoch 0 | Training loss = 4.512690603733063 | Test loss = 4.193840344746907\n",
    "Epoch 1 | Training loss = 3.207171857357025 | Test loss = 2.855196555455526\n",
    "Epoch 2 | Training loss = 2.788858652114868 | Test loss = 2.7461801369984946\n",
    "Epoch 3 | Training loss = 2.683992842833201 | Test loss = 2.7123541831970215\n",
    "Epoch 4 | Training loss = 2.7258232831954956 | Test loss = 2.6222329139709473\n",
    "Epoch 5 | Training loss = 2.623843808968862 | Test loss = 2.6298372745513916\n",
    "Epoch 6 | Training loss = 2.6232070525487265 | Test loss = 2.868154525756836\n",
    "Epoch 7 | Training loss = 2.6465203563372293 | Test loss = 2.767595052719116\n",
    "Epoch 8 | Training loss = 2.66033927599589 | Test loss = 2.743466377258301\n",
    "Epoch 9 | Training loss = 2.652188718318939 | Test loss = 2.6217918395996094\n",
    "Epoch 10 | Training loss = 2.60043211778005 | Test loss = 2.60316801071167\n",
    "Epoch 11 | Training loss = 2.5947894056638083 | Test loss = 2.6221278508504233\n",
    "Epoch 12 | Training loss = 2.578959862391154 | Test loss = 2.5901806354522705\n",
    "Epoch 13 | Training loss = 2.576880931854248 | Test loss = 2.594467878341675\n",
    "Epoch 14 | Training loss = 2.616027057170868 | Test loss = 2.591064214706421\n",
    "Epoch 15 | Training loss = 2.6497509876887 | Test loss = 2.662203232447306\n",
    "Epoch 16 | Training loss = 2.6083373626073203 | Test loss = 2.6001358032226562\n",
    "Epoch 17 | Training loss = 2.605268637339274 | Test loss = 2.5887749195098877\n",
    "Epoch 18 | Training loss = 2.6101978619893393 | Test loss = 2.6554483572642007\n",
    "Epoch 19 | Training loss = 2.6777352492014566 | Test loss = 2.6398793856302896\n",
    "Epoch 20 | Training loss = 2.7068386475245156 | Test loss = 2.7072757879892984\n",
    "Epoch 21 | Training loss = 2.627480169137319 | Test loss = 2.6925466855367026\n",
    "Epoch 22 | Training loss = 2.6190430919329324 | Test loss = 2.584099292755127\n",
    "Epoch 23 | Training loss = 2.566866397857666 | Test loss = 2.5893328189849854\n",
    "Epoch 24 | Training loss = 2.671197772026062 | Test loss = 2.942407210667928\n",
    "Epoch 25 | Training loss = 2.698890527089437 | Test loss = 2.6265974839528403\n",
    "Epoch 26 | Training loss = 2.62491504351298 | Test loss = 2.707094192504883\n",
    "Epoch 27 | Training loss = 2.680932104587555 | Test loss = 2.661949793497721\n",
    "Epoch 28 | Training loss = 2.6023327708244324 | Test loss = 2.5905651251475015\n",
    "Epoch 29 | Training loss = 2.5522359013557434 | Test loss = 2.57867161432902\n",
    "Epoch 30 | Training loss = 2.5873255928357444 | Test loss = 2.660228888193766\n",
    "Epoch 31 | Training loss = 2.6767356594403586 | Test loss = 2.644167423248291\n",
    "Epoch 32 | Training loss = 2.5707041025161743 | Test loss = 2.603447516759237\n",
    "Epoch 33 | Training loss = 2.572120189666748 | Test loss = 2.5676135222117105\n",
    "Epoch 34 | Training loss = 2.5445960760116577 | Test loss = 2.5792222817738852\n",
    "Epoch 35 | Training loss = 2.5703064997990928 | Test loss = 2.5740973154703775\n",
    "Epoch 36 | Training loss = 2.5989076296488443 | Test loss = 2.6521519819895425\n",
    "Epoch 37 | Training loss = 2.6060789426167807 | Test loss = 2.653982162475586\n",
    "Epoch 38 | Training loss = 2.5480491320292153 | Test loss = 2.56284761428833\n",
    "Epoch 39 | Training loss = 2.546206255753835 | Test loss = 2.582702080408732\n",
    "Epoch 40 | Training loss = 2.53329328695933 | Test loss = 2.5605823198954263\n",
    "Epoch 41 | Training loss = 2.5466638803482056 | Test loss = 2.6241183280944824\n",
    "Epoch 42 | Training loss = 2.5401841600735984 | Test loss = 2.554289976755778\n",
    "Epoch 43 | Training loss = 2.553957482179006 | Test loss = 2.613287925720215\n",
    "Epoch 44 | Training loss = 2.545074760913849 | Test loss = 2.5625597635904946\n",
    "Epoch 45 | Training loss = 2.5467688043912253 | Test loss = 2.565520763397217\n",
    "Epoch 46 | Training loss = 2.5416312416394553 | Test loss = 2.5513343016306558\n",
    "Epoch 47 | Training loss = 2.5294887820879617 | Test loss = 2.5814336140950522\n",
    "Epoch 48 | Training loss = 2.530319571495056 | Test loss = 2.6669086615244546\n",
    "Epoch 49 | Training loss = 2.5534868637720742 | Test loss = 2.567220369974772\n",
    "Epoch 50 | Training loss = 2.545904219150543 | Test loss = 2.5498883724212646\n",
    "Epoch 51 | Training loss = 2.519070307413737 | Test loss = 2.5748438040415444\n",
    "Epoch 52 | Training loss = 2.5424413681030273 | Test loss = 2.548670689264933\n",
    "Epoch 53 | Training loss = 2.5219553510348 | Test loss = 2.5572537581125894\n",
    "Epoch 54 | Training loss = 2.5094886223475137 | Test loss = 2.560319662094116\n",
    "Epoch 55 | Training loss = 2.511307974656423 | Test loss = 2.580026149749756\n",
    "Epoch 56 | Training loss = 2.5115503470102944 | Test loss = 2.5456807613372803\n",
    "Epoch 57 | Training loss = 2.5005688468615213 | Test loss = 2.547955592473348\n",
    "Epoch 58 | Training loss = 2.5198761622111 | Test loss = 2.6861111323038735\n",
    "Epoch 59 | Training loss = 2.5831815600395203 | Test loss = 2.637284517288208\n",
    "Epoch 60 | Training loss = 2.572909891605377 | Test loss = 2.562631607055664\n",
    "Epoch 61 | Training loss = 2.5002904136975608 | Test loss = 2.568574905395508\n",
    "Epoch 62 | Training loss = 2.4539151986440024 | Test loss = 2.6927483081817627\n",
    "Epoch 63 | Training loss = 2.4777592420578003 | Test loss = 2.6340691248575845\n",
    "Epoch 64 | Training loss = 2.552095274130503 | Test loss = 2.943647782007853\n",
    "Epoch 65 | Training loss = 2.5343843499819436 | Test loss = 2.6281658013661704\n",
    "Epoch 66 | Training loss = 2.511139174302419 | Test loss = 2.5869100093841553\n",
    "Epoch 67 | Training loss = 2.4871119459470115 | Test loss = 2.563323895136515\n",
    "Epoch 68 | Training loss = 2.5255517959594727 | Test loss = 2.5753180980682373\n",
    "Epoch 69 | Training loss = 2.462038576602936 | Test loss = 2.5954321225484214\n",
    "Epoch 70 | Training loss = 2.494799772898356 | Test loss = 2.5883309046427407\n",
    "Epoch 71 | Training loss = 2.426811178525289 | Test loss = 2.598881483078003\n",
    "Epoch 72 | Training loss = 2.4209784666697183 | Test loss = 2.7488608360290527\n",
    "Epoch 73 | Training loss = 2.46446826060613 | Test loss = 2.6066841284434\n",
    "Epoch 74 | Training loss = 2.468072255452474 | Test loss = 2.587620576222738\n",
    "Epoch 75 | Training loss = 2.3849696119626365 | Test loss = 2.6258848508199057\n",
    "Epoch 76 | Training loss = 2.3631367683410645 | Test loss = 2.606775919596354\n",
    "Epoch 77 | Training loss = 2.400774916013082 | Test loss = 2.6000608603159585\n",
    "Epoch 78 | Training loss = 2.334807813167572 | Test loss = 2.7142237027486167\n",
    "Epoch 79 | Training loss = 2.330035666624705 | Test loss = 2.6695587635040283\n",
    "Epoch 80 | Training loss = 2.2572935819625854 | Test loss = 2.6696623961130777\n",
    "Epoch 81 | Training loss = 2.2240594824155173 | Test loss = 2.754479726155599\n",
    "Epoch 82 | Training loss = 2.295351425806681 | Test loss = 2.61575714747111\n",
    "Epoch 83 | Training loss = 2.239086707433065 | Test loss = 2.7227691809336343\n",
    "Epoch 84 | Training loss = 2.207997610171636 | Test loss = 2.7276058991750083\n",
    "Epoch 85 | Training loss = 2.217043568690618 | Test loss = 2.6612442334493003\n",
    "Epoch 86 | Training loss = 2.2218828002611795 | Test loss = 2.7417481740315757\n",
    "Epoch 87 | Training loss = 2.2744336128234863 | Test loss = 2.69406795501709\n",
    "Epoch 88 | Training loss = 2.1858057777086892 | Test loss = 2.731642246246338\n",
    "Epoch 89 | Training loss = 2.1654090086619058 | Test loss = 2.6873814264933267\n",
    "Epoch 90 | Training loss = 2.127222160498301 | Test loss = 2.7915919621785483\n",
    "Epoch 91 | Training loss = 2.051015784343084 | Test loss = 2.6736954053243003\n",
    "Epoch 92 | Training loss = 2.0790861745675406 | Test loss = 2.634180943171183\n",
    "Epoch 93 | Training loss = 2.025949855645498 | Test loss = 2.751603285471598\n",
    "Epoch 94 | Training loss = 1.8675897320111592 | Test loss = 2.831928094228109\n",
    "Epoch 95 | Training loss = 1.9419245719909668 | Test loss = 2.8254872957865396\n",
    "Epoch 96 | Training loss = 1.8731929957866669 | Test loss = 2.7153898080190024\n",
    "Epoch 97 | Training loss = 1.8347639242808025 | Test loss = 2.763599236806234\n",
    "Epoch 98 | Training loss = 2.005356421073278 | Test loss = 2.7253236770629883\n",
    "Epoch 99 | Training loss = 1.8378906548023224 | Test loss = 2.7293864091237388\n",
    "Epoch 100 | Training loss = 1.8166932662328084 | Test loss = 2.7688406308492026\n",
    "Epoch 101 | Training loss = 1.7781188090642293 | Test loss = 2.717549959818522\n",
    "Epoch 102 | Training loss = 1.7398491303126018 | Test loss = 2.9368110497792563\n",
    "Epoch 103 | Training loss = 1.7139520943164825 | Test loss = 2.8264620304107666\n",
    "Epoch 104 | Training loss = 1.7621939480304718 | Test loss = 2.7718935012817383\n",
    "Epoch 105 | Training loss = 1.6212974886099498 | Test loss = 2.84261155128479\n",
    "Epoch 106 | Training loss = 1.559253732363383 | Test loss = 2.854212681452433\n",
    "Epoch 107 | Training loss = 1.4119056860605876 | Test loss = 2.96927809715271\n",
    "Epoch 108 | Training loss = 1.4034680724143982 | Test loss = 2.9812479813893638\n",
    "Epoch 109 | Training loss = 1.4534359474976857 | Test loss = 2.7374819119771323\n",
    "Epoch 110 | Training loss = 1.4576045274734497 | Test loss = 2.7759761810302734\n",
    "Epoch 111 | Training loss = 1.3911385635534923 | Test loss = 2.953104337056478\n",
    "Epoch 112 | Training loss = 1.3195232152938843 | Test loss = 2.8024230003356934\n",
    "Epoch 113 | Training loss = 1.2665107349554698 | Test loss = 2.9180691242218018\n",
    "Epoch 114 | Training loss = 1.2094118297100067 | Test loss = 2.8747410774230957\n",
    "Epoch 115 | Training loss = 1.2180902461210887 | Test loss = 2.9325674374898276\n",
    "Epoch 116 | Training loss = 1.0766840328772862 | Test loss = 2.876356840133667\n",
    "Epoch 117 | Training loss = 1.0015811175107956 | Test loss = 2.9061977863311768\n",
    "Epoch 118 | Training loss = 1.0212817738453548 | Test loss = 2.981333017349243\n",
    "Epoch 119 | Training loss = 1.015288641055425 | Test loss = 2.979546387990316\n",
    "Epoch 120 | Training loss = 0.9890188823143641 | Test loss = 2.894641160964966\n",
    "Epoch 121 | Training loss = 0.9872632374366125 | Test loss = 2.87300697962443\n",
    "Epoch 122 | Training loss = 0.8896637111902237 | Test loss = 2.922434409459432\n",
    "Epoch 123 | Training loss = 0.8756431738535563 | Test loss = 2.9475392500559487\n",
    "Epoch 124 | Training loss = 0.8730523735284805 | Test loss = 2.929404338200887\n",
    "Epoch 125 | Training loss = 0.8583066364129385 | Test loss = 2.959799289703369\n",
    "Epoch 126 | Training loss = 0.9471468329429626 | Test loss = 3.007989486058553\n",
    "Epoch 127 | Training loss = 0.8694671442111334 | Test loss = 2.8354373772939048\n",
    "Epoch 128 | Training loss = 0.793950746456782 | Test loss = 3.010768493016561\n",
    "Epoch 129 | Training loss = 0.7966095705827078 | Test loss = 2.877655824025472\n",
    "Epoch 130 | Training loss = 0.8992835581302643 | Test loss = 2.9013328552246094\n",
    "Epoch 131 | Training loss = 0.839479943116506 | Test loss = 2.894512971242269\n",
    "Epoch 132 | Training loss = 0.7980612516403198 | Test loss = 2.909008026123047\n",
    "Epoch 133 | Training loss = 0.7796344210704168 | Test loss = 2.997387647628784\n",
    "Epoch 134 | Training loss = 0.8155859808127085 | Test loss = 2.9685846964518228\n",
    "Epoch 135 | Training loss = 0.7698417057593664 | Test loss = 2.964077870051066\n",
    "Epoch 136 | Training loss = 0.653485839565595 | Test loss = 2.8753408590952554\n",
    "Epoch 137 | Training loss = 0.5882637848456701 | Test loss = 2.8730435371398926\n",
    "Epoch 138 | Training loss = 0.5726363807916641 | Test loss = 2.8583150704701743\n",
    "Epoch 139 | Training loss = 0.5824364374081293 | Test loss = 2.8840437730153403\n",
    "Epoch 140 | Training loss = 0.6097027460734049 | Test loss = 2.9701621532440186\n",
    "Epoch 141 | Training loss = 0.6112793187300364 | Test loss = 2.878804922103882\n",
    "Epoch 142 | Training loss = 0.6694032202164332 | Test loss = 2.877445936203003\n",
    "Epoch 143 | Training loss = 0.5986573696136475 | Test loss = 2.8629702726999917\n",
    "Epoch 144 | Training loss = 0.5496392001708349 | Test loss = 2.8680386543273926\n",
    "Epoch 145 | Training loss = 0.5609491864840189 | Test loss = 2.8797436555226645\n",
    "Epoch 146 | Training loss = 0.5688396791617075 | Test loss = 2.8715103467305503\n",
    "Epoch 147 | Training loss = 0.5568703909715017 | Test loss = 2.8297619024912515\n",
    "Epoch 148 | Training loss = 0.5489857320984205 | Test loss = 2.932460149129232\n",
    "Epoch 149 | Training loss = 0.5497378011544546 | Test loss = 2.8952627976735434\n",
    "Epoch 150 | Training loss = 0.5418885300556818 | Test loss = 2.8697479565938315\n",
    "Epoch 151 | Training loss = 0.5180931886037191 | Test loss = 2.817224899927775\n",
    "Epoch 152 | Training loss = 0.4848233411709468 | Test loss = 2.8670266469319663\n",
    "Epoch 153 | Training loss = 0.5095981508493423 | Test loss = 2.8884111245473227\n",
    "Epoch 154 | Training loss = 0.4795256058375041 | Test loss = 2.9046644369761148\n",
    "Epoch 155 | Training loss = 0.5045426338911057 | Test loss = 2.8604812622070312\n",
    "Epoch 156 | Training loss = 0.49995659043391544 | Test loss = 2.8988219102223716\n",
    "Epoch 157 | Training loss = 0.49334516127904254 | Test loss = 2.8423540592193604\n",
    "Epoch 158 | Training loss = 0.4781015043457349 | Test loss = 2.8826920986175537\n",
    "Epoch 159 | Training loss = 0.5287989204128584 | Test loss = 2.8374322255452475\n",
    "Epoch 160 | Training loss = 0.5487982630729675 | Test loss = 2.93006165822347\n",
    "Epoch 161 | Training loss = 0.4847252294421196 | Test loss = 2.8738359610239663\n",
    "Epoch 162 | Training loss = 0.4757622430721919 | Test loss = 2.890534003575643\n",
    "Epoch 163 | Training loss = 0.45206674685080844 | Test loss = 2.8310720125834146\n",
    "Epoch 164 | Training loss = 0.41246474782625836 | Test loss = 2.8837059338887534\n",
    "Epoch 165 | Training loss = 0.40487995743751526 | Test loss = 2.8537563482920327\n",
    "Epoch 166 | Training loss = 0.43476181974013645 | Test loss = 2.941516081492106\n",
    "Epoch 167 | Training loss = 0.44687868654727936 | Test loss = 2.8688937028249106\n",
    "Epoch 168 | Training loss = 0.4426387498776118 | Test loss = 2.888704538345337\n",
    "Epoch 169 | Training loss = 0.4189794734120369 | Test loss = 2.8439768155415854\n",
    "Epoch 170 | Training loss = 0.3893149172266324 | Test loss = 2.864601214726766\n",
    "Epoch 171 | Training loss = 0.40359241763750714 | Test loss = 2.9454919497172036\n",
    "Epoch 172 | Training loss = 0.41057723263899487 | Test loss = 2.8415912787119546\n",
    "Epoch 173 | Training loss = 0.36598721891641617 | Test loss = 2.8684465090433755\n",
    "Epoch 174 | Training loss = 0.3647552008430163 | Test loss = 2.8665629227956138\n",
    "Epoch 175 | Training loss = 0.3340824767947197 | Test loss = 2.858413060506185\n",
    "Epoch 176 | Training loss = 0.3284987856944402 | Test loss = 2.866041421890259\n",
    "Epoch 177 | Training loss = 0.36035139113664627 | Test loss = 2.8429741859436035\n",
    "Epoch 178 | Training loss = 0.3578016261259715 | Test loss = 2.8614677588144937\n",
    "Epoch 179 | Training loss = 0.33256562054157257 | Test loss = 2.8851611614227295\n",
    "Epoch 180 | Training loss = 0.3387721429268519 | Test loss = 2.8600749174753823\n",
    "Epoch 181 | Training loss = 0.3714219356576602 | Test loss = 2.88550074895223\n",
    "Epoch 182 | Training loss = 0.3537074451645215 | Test loss = 2.857067267100016\n",
    "Epoch 183 | Training loss = 0.34837787101666134 | Test loss = 2.838027000427246\n",
    "Epoch 184 | Training loss = 0.3505762368440628 | Test loss = 2.842094580332438\n",
    "Epoch 185 | Training loss = 0.38562504450480145 | Test loss = 2.835160414377848\n",
    "Epoch 186 | Training loss = 0.33873580644528073 | Test loss = 2.8579559326171875\n",
    "Epoch 187 | Training loss = 0.3242340013384819 | Test loss = 2.873668988545736\n",
    "Epoch 188 | Training loss = 0.3271553839246432 | Test loss = 2.8571038246154785\n",
    "Epoch 189 | Training loss = 0.31058179835478467 | Test loss = 2.8258886337280273\n",
    "Epoch 190 | Training loss = 0.3144623264670372 | Test loss = 2.8323076566060386\n",
    "Epoch 191 | Training loss = 0.31062446782986325 | Test loss = 2.8996430238087973\n",
    "Epoch 192 | Training loss = 0.33441001425186795 | Test loss = 2.8261852264404297\n",
    "Epoch 193 | Training loss = 0.3811745047569275 | Test loss = 2.925759792327881\n",
    "Epoch 194 | Training loss = 0.3513134370247523 | Test loss = 2.856234312057495\n",
    "Epoch 195 | Training loss = 0.37271806846062344 | Test loss = 2.865464528401693\n",
    "Epoch 196 | Training loss = 0.3859856625398 | Test loss = 2.845203479131063\n",
    "Epoch 197 | Training loss = 0.388036107023557 | Test loss = 2.8866395950317383\n",
    "Epoch 198 | Training loss = 0.3276962439219157 | Test loss = 2.890552282333374\n",
    "Epoch 199 | Training loss = 0.3188909739255905 | Test loss = 2.821582555770874\n",
    "Epoch 200 | Training loss = 0.3096032291650772 | Test loss = 2.8967040379842124\n",
    "Epoch 201 | Training loss = 0.3415669451157252 | Test loss = 2.789390484491984\n",
    "Epoch 202 | Training loss = 0.3384142865737279 | Test loss = 2.926778554916382\n",
    "Epoch 203 | Training loss = 0.31698766350746155 | Test loss = 2.837609132130941\n",
    "Epoch 204 | Training loss = 0.29759829118847847 | Test loss = 2.8420074780782065\n",
    "Epoch 205 | Training loss = 0.27972996855775517 | Test loss = 2.8413052558898926\n",
    "Epoch 206 | Training loss = 0.2958216095964114 | Test loss = 2.905440409978231\n",
    "Epoch 207 | Training loss = 0.2879531954725583 | Test loss = 2.8942856788635254\n",
    "Epoch 208 | Training loss = 0.3514970491329829 | Test loss = 2.8464094003041587\n",
    "Epoch 209 | Training loss = 0.3302631974220276 | Test loss = 2.8374098936716714\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f333c6-a3e6-491a-b71c-d9ddae7c8b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95018b-5f35-43fb-96b4-4289d5078dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b856b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
